{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd076864-ade9-479c-92b0-233e8da31928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.collections import EllipseCollection\n",
    "from sklearn.preprocessing import scale\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4bbaf-0e38-4325-a6e2-95602b3a2c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_corr_ellipses(data, ax=None, **kwargs):\n",
    "\n",
    "    M = np.array(data)\n",
    "    if not M.ndim == 2:\n",
    "        raise ValueError('data must be a 2D array')\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, subplot_kw={'aspect':'equal'})\n",
    "        ax.set_xlim(-0.5, M.shape[1] - 0.5)\n",
    "        ax.set_ylim(-0.5, M.shape[0] - 0.5)\n",
    "\n",
    "    # xy locations of each ellipse center\n",
    "    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n",
    "\n",
    "    # set the relative sizes of the major/minor axes according to the strength of\n",
    "    # the positive/negative correlation\n",
    "    w = np.ones_like(M).ravel()\n",
    "    h = 1 - np.abs(M).ravel()\n",
    "    a = 45 * np.sign(M).ravel()\n",
    "\n",
    "    ec = EllipseCollection(widths=w, heights=h, angles=a, units='x', offsets=xy,\n",
    "                           transOffset=ax.transData, array=M.ravel(), **kwargs)\n",
    "    ax.add_collection(ec)\n",
    "\n",
    "    # if data is a DataFrame, use the row/column names as tick labels\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        ax.set_xticks(np.arange(M.shape[1]))\n",
    "        ax.set_xticklabels(data.columns, rotation=90)\n",
    "        ax.set_yticks(np.arange(M.shape[0]))\n",
    "        ax.set_yticklabels(data.index)\n",
    "\n",
    "    return ec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0cafc-0379-4d2d-b4de-a5c4da01f953",
   "metadata": {},
   "source": [
    "Dans ce projet, nous allons nous intérésser au chargement des stations de vélibs de la ville de Paris. Nous allons pour ce faire considérer les jeux de données 'velibLoading.csv' ainsi que 'velibCoord.csv'. Le premier data frame contient les chargements des différentes sations de vélibs heure par heure au cours d'une semaine. Ce data frame a donc pour variable (=colonne) les Chargements des stations à une heure précise d'un jour de la semaine et pour individus (=lignes), les différentes stations de la ville de Paris que nous considérons.Le chargement des stations est défini comme le quotient/rapport du nombre de vélib disponible dans la station par le nombre de vélib total. On comprend donc que si le chargement vaut 1, la station est rempli de vélib, tandis que, si le chargement vaut 0, la station est vide car ils sont tous actuellement en cours d'utilisation.\n",
    "Le data frame 'velibCoord.csv' quant à lui contient la longitude de la station, la latitude de la station, le Bonus et le nom de la station en question.Les individus sont les stations de la ville de Paris.\n",
    "On considèrera 1189 individus, c'est à dire, 1189 stations de vélib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcaeb3-d624-4fa3-8879-c552e0d0c0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Chargement du jeu de donnée loading\n",
    "loading = pd.read_csv('velibLoading.csv', delimiter=' ')\n",
    "print(loading.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e23d7a-acc1-4c02-a149-48d712c75874",
   "metadata": {},
   "source": [
    "Le data frame loading est de taille (1189,168). Comme expliqué précedemment, les 1189 individus correspondent aux stations de vélib. Les 168 colonnes viennent des tranches horaires que l'on a utilisé pour décomposer la semaine en tranche horaire de 1heure. On a 24h dans une journée et 7 jours dans la semaine, et si l'on fait 24x7 on obtient 168h donc on a 168 colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a63bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loading.mean(axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d8220-dee8-44d3-9a46-328741cb5247",
   "metadata": {},
   "source": [
    "La commande que nous venons d'utiliser nous donne des informations intéréssantes qui nous seront très utiles par la suite. Tout d'abord, nous voyons qu'il y a 1189 stations de vélibs dans la ville de Paris et que la moyenne en chargement de vélib au cours de la semaine dans la ville de Paris est de 0.38 ( 38%) qui est inférieur à 0.5. Cela signifie que les vélibs des stations sont en moyenne chargé en vélib à 38%, donc en moyenne, 72% des vélibs sont utilisés. Cela nous montre que ce moyen de transport est tout de même très utilisé dans la ville de Paris.\n",
    "L'écart type nous permet de constater que le chargement en vélib des stations varie d'environ 0.212 par rapport à la moyenne, signfiant que les stations ont un chargement en vélib assez proche de la moyenne en chargement de vélib de la ville de Paris. \n",
    "Le chargement minimal de 0.016 nous indique qu'au cours de la semaine certaines stations peuvent avoir une disponibilité très faible en vélib allant jusqu'à  1.6%, signifiant qu'environ 98% des vélibs sont en cours d'utilisation. On peut imaginer que ce cas de figure se produit dans certaines stations aux heures où les parisiens doivent se rendre sur leur lieux de travail.\n",
    "Le chargement maximal de 0.919 nous montre qu'au contraire certaines stations peuvent avoir un remplissage assez conséquent en vélib au cours de la semaine. Ce qui est intéréssant à noter est que le maximum est différent de 1. Cela nous montrer qu'au cours de la semaine, il n'y a aucune station qui est chargée à 100% en vélib.\n",
    "Le premier quartile nous indique que 25% des stations ont un chargement inférieur à 0.20. La médiane nous informe que la moitié des stations de Paris ont un chargement moyen inférieur à 0.35 < 0.5. Le troisième quartile quant à lui nous permet d'observer que 75% des stations de vélibs de Paris ont un chargement inférieur à 0.53.\n",
    "Cela nous montre que ce moyen de transport est tout de même très utilisé dans la ville de Paris.\n",
    "Dans la suite nous nous pencherons sur un recodage des variables pour l'étude de l'altitude des stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e6994-6e93-43bf-bea5-261ab1de1f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chargement du jeu de donnée coord\n",
    "coord = pd.read_csv('velibCoord.csv',delimiter=' ')\n",
    "coord.head()\n",
    "#print(coord.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f5ad0a-c0ff-4994-9530-0a22d97796de",
   "metadata": {},
   "source": [
    "Le data frame coord est de taille (1189,4).  Comme expliqué précédemment, les 1189 individus correspondent aux stations de vélibs. Dans ce data frame, la variable names nous informe du nom des stations de vélibs des individus, nous retrouvons également la longitude et latitude nous permettant de connaitre la position des différentes stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfde69-6af8-4281-9464-8be1ac384933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat='latitude', lon='longitude',\n",
    "                        mapbox_style='open-street-map',\n",
    "                        zoom=10, opacity=.9,\n",
    "                        title='Stations de vélibs à Paris',\n",
    "                        color_discrete_sequence=['black'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e96d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définir les intervalles et les étiquettes pour chaque catégorie\n",
    "intervals = [0, 0.204574, 0.353352, 0.532460, 0.7, 1.1]\n",
    "labels = ['très peu chargé', 'peu chargé', 'moyen', 'chargé', 'très chargé']\n",
    "\n",
    "# Appliquer la transformation\n",
    "categorical_data = pd.DataFrame()\n",
    "for column in loading.columns:\n",
    "    categorical_data[column] = pd.cut(loading[column], bins=intervals, labels=labels, right=False)\n",
    "\n",
    "# Afficher les premières lignes du nouveau DataFrame\n",
    "categorical_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576dc27-a04c-4492-9921-7f40addee8e1",
   "metadata": {},
   "source": [
    "Dans ce projet, nous allons tout d'abord réaliser une analyse descriptive de nos données. Ensuite, nous ferons une une analyse en composante principal(ACP) au vu de la grande dimension de nos données. Par la suite, nous nous pencherons sur les méthodes de clustering tel que K-means, GMM. L'objectif est de détecter des \"clusters\" afin de pouvoir regrouper les utilisateurs de vélibs selon l'utilisation qu'ils ont. Ces clusters nous permettrons ensuite de prédire les chargements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9957c-5c40-41b9-a131-7d01e4202d9b",
   "metadata": {},
   "source": [
    "# Etude rapide du jeu de donnée( Vérification des données)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46b181-f7a7-47f2-9345-21f7ed8231ea",
   "metadata": {},
   "source": [
    "Nous allons procéder dans cette partie à une vérification de la validité de nos jeux de données loading et coord. \n",
    "Dans un premier temps, nous vérifions que toutes les cases de nos jeu de données sont remplis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b79e0-7179-4981-8c2d-5a5acfdbb85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Verification présence de case non remplis dans les 2 jeux de données\n",
    "print(loading.any().isna().sum().sum())\n",
    "#POur chacune des colonnes,on verifie si il y a la présnce d'une case non rempli (NA), après on somme les cases NA de cette colonne\n",
    "#Ensuite on somme les sommes des colonnes. Le résultat retourné pour le jeux de donnée Loading est 0.\n",
    "#Il n'a donc pas de donnée manquante dans Loading\n",
    "print(coord.any().isna().sum().sum())\n",
    "# De même pour Coord."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd69714-ba87-49fd-a16c-ee2ebc674e38",
   "metadata": {},
   "source": [
    "On vérifie donc que pour chacune des colonnes s'il y a la présence d'une case non rempli (NA), et l'on somme ces potentiels case non remplis de cette colonne, puis on sommes les sommes obtenus pour chaque colonne. Le résultat retourné est 0 pour nos deux jeux de données nous indiquant bien que nos jeux de données ne présentent pas de valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b97489-ba2b-4ffc-9957-650d79c88a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Verification ligne dupliquées dans les jeux de données\n",
    "print(loading.duplicated().sum())\n",
    "#Ici, on va parcourir les lignes et vérifié s'il y a des lignes dupliquées. Ici ce n'est pas le cas.\n",
    "print(coord.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5361157-e7b7-437a-8a73-9a04197d2fa7",
   "metadata": {},
   "source": [
    "Dans cette cellule, nous effectuons une verification de potentielles lignes dupliquées de nos jeux de données. Ici nous retournons 0 pour nos deux jeux de données nous confirmant la non présence de lignes dupliquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19657a7-3522-4018-ab82-c484e7583874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Etude nom des stations\n",
    "#print(coord['names'])\n",
    "coord['names'].duplicated().sum()\n",
    "#Cela retourne 28\n",
    "duplicated_names = coord[coord['names'].duplicated()]\n",
    "\n",
    "# Afficher les noms des stations identiques\n",
    "print(\"Noms des stations identiques :\")\n",
    "print(duplicated_names['names'].unique())\n",
    "#print(duplicated_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a95cc-327f-446a-aba1-d73623e9dc19",
   "metadata": {},
   "source": [
    "A présent, nous allons nous pencher sur le nom des stations de vélibs de notre jeu de données coord. Il y'a donc 28 noms de stations qui se répètent, cela s'explique par le fait que certaines stations portent le même nom,. Nous décidons d'afficher le nom de ces stations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580bc23-b27f-4ee2-a78b-e8b3c914e8b4",
   "metadata": {},
   "source": [
    "Nous avons donc bien compris et vérifié la véracité ainsi que la complétude de nos données. Nous allons mainteant nous pencher sur l'analyse descriptive de nos données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46cfc8-d2b0-43a9-8f67-8e5e8945ebde",
   "metadata": {},
   "source": [
    "# Etude du remplissage des stations selon l'heure ou le jour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be4167-0009-4b44-8ffa-675c35373d75",
   "metadata": {},
   "source": [
    "A présent nous allons nous pencher sur le remplissage des stations selon l'heure de la journée ainsi que selon le jour de la semaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5960a83-0b32-4d3d-9ba5-9baf2f8192c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Evolution de la disponibilité à la station Quai de la RAPEE au cours de la semaine.\n",
    "i = 4\n",
    "loading_data = loading.to_numpy()\n",
    "print(coord)\n",
    "n_steps = len(loading_data[i])   # number of observed time steps\n",
    "time    = 24*7  # observed time range\n",
    "\n",
    "plt.figure(figsize = (20, 6))\n",
    "\n",
    "plt.plot(loading_data[i])\n",
    "for j in range(1,7*24+1):\n",
    "    if j%24==0:\n",
    "        plt.vlines(x=j,colors='r',ymin=0,ymax=1)\n",
    "plt.xlabel('Heure')\n",
    "plt.ylabel('Ratio du nombre de vélib disponible')\n",
    "plt.title(coord['names'][i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809fa5d-c0fd-455f-a5a0-915110a5fbbd",
   "metadata": {},
   "source": [
    "Ce graphique nous montre le remplissage de la station de vélib Quai de la RAPEE au cours de la semaine. Nous avons pris soin de séparer les jours de la semaine à l'aide de lignes rouges.\n",
    "On observe que le chargement de la station est périodique au cours de la semaine et que la tendance est différente à la fin de la semaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc07d1-2efa-466f-ab53-8b207b6d5ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Evolution de la disponibilité pour 16 stations au cours de la semaine.\n",
    "#plt.figure(figsize = (1000, 70))\n",
    "a=3; b=3;\n",
    "time=[24*(1+j) for j in range(7)]\n",
    "fig, ax = plt.subplots(a, b,figsize=(30,20), layout='constrained')\n",
    "fig.suptitle(\"Affichage de l'évolution de Ratio disponibilité des vélibs pour 16 stations\")\n",
    "for i in range(a):\n",
    "    for j in range(b):\n",
    "        k= np.random.choice(np.array([l for l in range(len(loading_data[:,1]))]))\n",
    "        ax[i,j].plot(loading_data[k,:])\n",
    "        ax[i,j].set_title(coord['names'][k])\n",
    "        ax[i,j].vlines(time,0,1,color=\"black\")\n",
    "for ax in ax.flat:\n",
    "    ax.set(xlabel='Heure', ylabel='Ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3cc499-5fa0-459c-af7a-217e29dd1091",
   "metadata": {},
   "source": [
    "Nous remaquons que les chargements des stations dépendent certes des jours de la semaine( en semaine les chargement sont différents de ceux du week end) mais que cela dépend bien de la station en elle même. Si on prend 2 stations différentes, on aura 2 dynamiques de chargement différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be02d6-93f0-4c34-a6b0-298bb58eaefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,9))\n",
    "n_steps    = loading.shape[1]  # number of observed time steps\n",
    "time_range = np.linspace(1, n_steps, n_steps)  # observed time range\n",
    "time_tick  = np.linspace(1, n_steps, 8) \n",
    "\n",
    "\n",
    "bp = plt.boxplot(loading_data, widths = 0.75, patch_artist = True)\n",
    "\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.8)\n",
    "    \n",
    "for median in bp['medians']:\n",
    "    median.set(color = \"Purple\", linewidth=5)\n",
    "    \n",
    "\n",
    "    \n",
    "plt.vlines(x = time_tick, ymin = 0, ymax = 1, \n",
    "           colors = \"Orange\", linestyle = \"dotted\", linewidth = 5)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Heures', fontsize = 20)\n",
    "plt.ylabel('Chargement en vélibs', fontsize = 20)\n",
    "plt.title(\"Boxplots des chargements horaires au cours de la semaine\", fontsize = 25)\n",
    "plt.xticks(ticks = np.arange(0, 168, 5), labels=np.arange(0, 168, 5), fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593253e-3d62-4922-a82b-a9c65972c3c7",
   "metadata": {},
   "source": [
    "Ces boxplots représentent le chargement moyen des stations au cours de la semaine. Tout comme pour le graphique précedent, nous séparons les jours de la semaine à l'aide des lignes oranges pour des questions de lisibilité et de simplicité d'interprétation. La courbe violette représente la médiane, l'allure de cette courbe nous permet de comprendre le chargement jouralier et donc hebdomadaire du chargement global des stations de la ville de Paris. Nous pouvons observer un maximum de disponibilité vers 08h et un minimum de disponibilité vers 19h pour les différents jours de la semaine. Le week end le maximum est un peu plus tard dans la journée. \n",
    "Nous savons que pour les boxplots, il y'a une ligne pour le 1er quartile , une seconde pour le 2nd quartile ( médiane) et une troisième pour le troisième quartile. Nous observons une disymétrie des données par rapport à la médiane. En effet, si l'on remaque que la différence entre le troisième quartile et la médiane est plus grande que la différence entre le premier quartile est la médiane reflettant bien la disymétrie des données. Cela confirme les résultats obtenu au début à l'aide de la commande python describe. En effet, on avait vu que :\n",
    "la moyenne était de 0.381622, le premier quartile était de 0.204574, la médiane était de 0.353352 et le troisième quartile était de 0.532460. Et la différence médiane premier quartile est d'environ 0.15, ce qui est inférieure à la différence entre le troisième quartile et la médiane étant d'environ 0.18 et 0.15<0.18.\n",
    "D'autre part la dispersion des données à l'air constante en semaine car les courbes on à peu prés la même allure. Cependant, on observe que le samedi, le ratio de disponibilité des données dépasse 0.8. Et que le dimanche l'allure de la disponibilité est proche du samedi mais avec des valeurs de chargement plus proche de celle des jours de la semaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc65ca-3d42-4a96-a0ba-5785ad761734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_per_hour_per_day = loading.mean(axis = 0).to_numpy()\n",
    "mean_per_hour_per_day = mean_per_hour_per_day.reshape((7, 24))\n",
    "\n",
    "mean_per_hour = mean_per_hour_per_day.mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "days = [\"Monday\", \"Tuesday\", \"Wednesday\",\"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "plt.plot(mean_per_hour_per_day.transpose())\n",
    "plt.plot(mean_per_hour, color = \"black\", linewidth = 3)\n",
    "\n",
    "plt.xlabel('Hourly loading, averaged over all stations', fontsize = 20)\n",
    "plt.ylabel('Loading', fontsize = 20)\n",
    "plt.legend(days + ['Weekly'])\n",
    "plt.xticks(ticks = np.arange(0,24,4), labels=np.arange(0,24,4), fontsize = 15)\n",
    "  \n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120301d",
   "metadata": {},
   "source": [
    "On affiche le chargement moyen de chaque jour de la semaine, on observe que les jours de la semaine suivent la même tendance qui est assez différente par rapport à celle des jours de weekend.\n",
    "On observe 2 pics de chargement faible à 9h et à 19h, cela corred=spond aux heures où les gens partent au travail et rentrent du travail (heures du transit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f31fbe-1f3e-4386-b9a4-43cdbb43068e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(loading.mean())\n",
    "plt.figure(0)\n",
    "plt.plot(loading.mean(axis=0))\n",
    "plt.axhline(y = np.mean(loading.mean(axis=0)), color = 'r')\n",
    "plt.title(\"Average loading by stations\")\n",
    "#moyenne sur les stations\n",
    "plt.figure(1)\n",
    "plt.plot(loading.mean(axis=1))\n",
    "plt.axhline(y = np.mean(loading.mean(axis=1)), color = 'r')\n",
    "plt.title(\"Average hourly loading\")\n",
    "#moyenne sur le temps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70aef15-d7c3-4263-906b-d5a0b94dc404",
   "metadata": {},
   "source": [
    "Le second graphe représente le chargement moyen en vélibs pour les stations au cours de la semaine, chacune des barres nous indique la moyenne en chargement de vélib de la station au cours de la semaine. Ce graphe nous confirme ce que l'on voyait précédemment puisque l'on observe que certaines stations ont un chargement moyen très fort indiquant que leurs vélibs ne sont pas beaucoup utilisés. Tandis que d'autres stations ont un chargement moyen assez faible montrant que ce sont des stations qui voient leurs vélibs être fortement utilisés par les parisiens. De plus, nous voyons que la ligne rouge correspondant à la  moyenne en chargement de la moyenne de chargement des stations est en dessous de 0.4 nous indiquant que les stations ont géneralement plus de places libres que de places occupées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2056a-8ce6-4542-af82-4e55ccb334a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## Simple 2D representation\n",
    "# Loading at 6pm, depending on the day of the week\n",
    "\n",
    "h = 18\n",
    "# h =0\n",
    "#h=12\n",
    "hours = np.arange(h, 168, 24)\n",
    "\n",
    "load_per_hour = loading_data[:, hours]\n",
    "\n",
    "days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "# --- #\n",
    "\n",
    "s, m = 10, 3\n",
    "k = 1 + len(days)//m\n",
    "\n",
    "fig = plt.figure(figsize=(s+1, s))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=.3, wspace=.25)\n",
    "\n",
    "for (i,d) in enumerate(days):\n",
    "    ax = fig.add_subplot(k, m, i+1)\n",
    "    im = ax.scatter(coord.longitude, coord.latitude, c = load_per_hour[:,i])\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    ax.set_title('Stations loading - ' + d + ' {} h'.format(h))\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.tick_params(axis='x')\n",
    "    ax.tick_params(axis='y')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae362f8e-abd9-4c30-b887-ad9fa84b799c",
   "metadata": {},
   "source": [
    "Ces graphiques nous montrent le remplissage moyen des différentes stations de Paris à une heure précise de la semaine. La latitude et la longitude nous permette de savoir ou sont situées ces stations dans la ville de Paris. Ce qui est flagrant sur ces différents graphiques c'est que les chargements ont à peu près la même tendance à différentes heures de la journée, à différents jours de la semaine. En effet, nous voyons très clairement une forme d'arc de cercle/banane des stations au centre. Ces stations sont celles qui possèdent le taux de disponibilité le plus haut. Le fait que ces stations ont un grand taux de disponibilité peu peut être s'expliquer par une utilisation d'autres transports en commun dans ces zones, tel que le métro ou une difficulté à circuler dans ces zones avec un vélib.\n",
    "Alors que les stations se trouvant au nord et au sud ont quant à elles des taux de remplissage beaucoup plus faible refletant la forte utilisation de vélib dans ces endroits de la ville de Paris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05949c-6922-433c-92ec-843845256aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Chargement moyen des stations à 18h\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import plotly.express as px\n",
    "h = 18\n",
    "loading_data=loading.to_numpy()\n",
    "hours = np.arange(h, 168, 24)\n",
    "load_per_hour = loading_data[:, hours].mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = load_per_hour, color_continuous_scale = px.colors.sequential.Plasma_r, #size = load_per_hour,\n",
    "                        zoom  = 10, opacity = .9,\n",
    "                        title = 'Stations loading - Weekly average at {} h'.format(h))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c5262-5880-4775-ac4b-3f7b6581cb50",
   "metadata": {},
   "source": [
    "Nous pouvons représenter les ratios des disponibilités des différentes stations de la ville de Paris sur une carte intéractive. Cette carte nous permet de voir l'emplacement exacte dans la ville des différentes stations. De plus, si l'on clique sur une station, nous voyons directement sa lattitude, sa longitude ainsi que son chargement moyen à 18h.\n",
    "Nous observons que les stations sont rassemblées en sorte de groupes selon leur chargement. Mise à part certaines stations que nous pouvons interpréter comme étant des outliers, nous voyons que les stations ayant des remplissages similaires forment des regroupements. Nous reviendrons sur l'étude de ces groupes de couleurs formés par les stations lors de la partie sur le clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30578c9-a965-4a2d-b271-1bca4124232b",
   "metadata": {},
   "source": [
    "# Decomposition du data frame loading en jours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438860a-d860-4c76-a4e5-e3e260dd17ee",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous commençons par créer 7 sous data frame à partir de notre data frame initial loading. On crée un data frame pour chaque jour de la semaine. Les dimensions de 7 sous data frame sont donc de taille (24,1168), car on regroupe les 24h d'une journée à l'intérieur de ce data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdc86d-3eae-4d99-93c9-ca942d4e3597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lundi = loading.iloc[:, :24]  \n",
    "mardi = loading.iloc[:, 24:48] \n",
    "mercredi = loading.iloc[:, 48:72] \n",
    "jeudi = loading.iloc[:, 72:96] \n",
    "vendredi = loading.iloc[:, 96:120] \n",
    "samedi = loading.iloc[:, 120:144] \n",
    "dimanche = loading.iloc[:, 144:168] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loading.groupby(loading.columns.str.split('-').str[0], axis=1).mean()\n",
    "\n",
    "ordre_jours = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam','Dim']\n",
    "df = df.reindex(columns=ordre_jours)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f3d6f-2b54-4c55-abf4-0716c6137ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "corr = df.corr( )\n",
    "\n",
    "# Affichage de la matrice de corrélation\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "m = plot_corr_ellipses(corr, ax=ax, cmap='seismic',clim=[0, 1])\n",
    "cb = fig.colorbar(m)\n",
    "cb.set_label('Correlation coefficient')\n",
    "ax.margins(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f924a9-7dd4-4f86-827f-71dd95051c42",
   "metadata": {},
   "source": [
    "Cette figure représente la matrice des corrélation. On observe très clairement une corrélation forte en samedi et dimanche due au fait que ce sont les jours du week end, impliquant potentionnellement une dynamique d'utilisation des vélibs différentes des autres jours de la semaine. D'autres corrélations de cette intensité sont celle entre mardi et mercredi et celle entre jeudi et vendredi.\n",
    "On voit bien la différence de corrélation entre les jours de la semaine, ceux du week end sont très corrélés entre eux et peu corrélés avec les jours de la semaine. Tandis que, ceux de la semaine sont très corrélés entre eux et peu corrélés avec les jours du week end.\n",
    "Les vélbs sont utilisés aux mêmes horaires  tous les jours sauf qu'en week end ils sont moins utilisés, ce qui explique que l'on a une corrélation un peu moins forte entre la semaine et le week end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf93a1-3d2d-41a6-9ad2-2a53848ee26e",
   "metadata": {},
   "source": [
    "# Decomposition de lundi en Lundi Nuit et Lundi Journée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5914c-d94a-4170-ba51-a1a8e464d567",
   "metadata": {},
   "source": [
    "A présent, nous allons à partir de chacun des 7 nouveaux sous data frame créer deux nouveaux sous data frame qui seront un data frame contenant les heures de journée et un data frame contenant les heures de nuit. Nous choisissons arbitrairement ces tranches horaires et choisissons comme  de prendre comme convention: 00h à 07h59 et 20h00 à 00h comme heures de nuit et les heures restantes (08h00 à 19h59) comme heures de journée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f93da9-ebf9-41a7-ac96-72e4c112cb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sélectionner les colonnes pour les heures de nuit (de 00h à 07h59 et 20h00 à 00h)\n",
    "lundi_nuit = pd.concat([lundi.iloc[:, 0:8], lundi.iloc[:, 20:24]], axis=1)\n",
    "\n",
    "# Sélectionner les colonnes pour les heures de journée (de 08h00 à 19h59)\n",
    "lundi_journée = lundi.iloc[:, 8:20]\n",
    "\n",
    "# Afficher les premières lignes du dataframe 'lundi_nuit'\n",
    "print(\"Lundi Nuit:\")\n",
    "print(lundi_nuit.head())\n",
    "\n",
    "# Afficher les premières lignes du dataframe 'lundi_journée'\n",
    "print(\"\\nLundi Journée:\")\n",
    "print(lundi_journée.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d696019-ce84-4a3d-bd34-318e9c725901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pour mardi\n",
    "mardi_nuit = pd.concat([mardi.iloc[:, 0:8], mardi.iloc[:, 20:24]], axis=1)\n",
    "mardi_journée = mardi.iloc[:, 8:20]\n",
    "\n",
    "#Pour mercredi\n",
    "mercredi_nuit = pd.concat([mercredi.iloc[:, 0:7], mercredi.iloc[:, 20:24]], axis=1)\n",
    "mercredi_journée = mercredi.iloc[:, 8:20]\n",
    "#Pour jeudi\n",
    "jeudi_nuit = pd.concat([jeudi.iloc[:, 0:7], jeudi.iloc[:, 20:24]], axis=1)\n",
    "jeudi_journée = jeudi.iloc[:, 8:20]\n",
    "#Pour vendredi\n",
    "vendredi_nuit = pd.concat([vendredi.iloc[:, 0:7], vendredi.iloc[:, 20:24]], axis=1)\n",
    "vendredi_journée = vendredi.iloc[:, 8:20]\n",
    "#Pour samedi\n",
    "samedi_nuit = pd.concat([samedi.iloc[:, 0:7], samedi.iloc[:, 20:24]], axis=1)\n",
    "samedi_journée = samedi.iloc[:, 8:20]\n",
    "#Pour dimanche\n",
    "dimanche_nuit = pd.concat([dimanche.iloc[:, 0:7], dimanche.iloc[:, 20:24]], axis=1)\n",
    "dimanche_journée = dimanche.iloc[:, 8:20]\n",
    "\n",
    "print(vendredi_journée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1bbdb-632a-4497-9481-224b8d73c33b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Liste des dataframes à fusionner\n",
    "dataframes = [lundi_nuit, lundi_journée, mardi_nuit, mardi_journée, mercredi_nuit, mercredi_journée,\n",
    "               jeudi_nuit, jeudi_journée, vendredi_nuit, vendredi_journée, samedi_nuit, samedi_journée,\n",
    "               dimanche_nuit, dimanche_journée]\n",
    "\n",
    "# Fusionner tous les dataframes en un seul dataframe\n",
    "result = pd.concat(dataframes, keys=['lundi_nuit', 'lundi_journée', 'mardi_nuit', 'mardi_journée', 'mercredi_nuit',\n",
    "                                     'mercredi_journée', 'jeudi_nuit', 'jeudi_journée', 'vendredi_nuit',\n",
    "                                     'vendredi_journée', 'samedi_nuit', 'samedi_journée', 'dimanche_nuit',\n",
    "                                     'dimanche_journée'], axis=1)\n",
    "\n",
    "# Réinitialiser les index\n",
    "result.reset_index(inplace=True)\n",
    "res = result.drop(columns=['index'])\n",
    "\n",
    "# Afficher le résultat\n",
    "jours = res.groupby(level=0, axis=1).mean()\n",
    "\n",
    "# Calcul de la corrélation\n",
    "\n",
    "corr_jours = jours.corr()\n",
    "\n",
    "# Affichage de la matrice de corrélation\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "m = plot_corr_ellipses(corr_jours, ax=ax, cmap='seismic',clim=[0, 1])\n",
    "cb = fig.colorbar(m)\n",
    "cb.set_label('Correlation coefficient')\n",
    "ax.margins(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e025dc3-9622-44ac-99b0-2c4780180345",
   "metadata": {},
   "source": [
    "Nous allons à présent nous pencher sur la matrice des corrélations, toutefois, nous prendrons comme variables les horaires de nuit et de journée pour chacun des jours. \n",
    "Tout d'abord, nous observons qu'il y a des corrélations fortes entre les différents horaires de journée de la semaine. En effet, on ressent bien que la dynamique d'utilisation des vélibs est similaires durant les horaires de journée au cours des différents jours de la semaine. Cependant, on notera toutefois que l'on retrouve une corrélation assez particulière pour dimanche. En effet, dimnanche journée est très corrélé avec dimanche nuit et très corrélé avec samedi journée et nuit. Nous avions déja expliqué cette corrélation. Le fait qu'il n'y ait pas de grande différence entre les corrélations liées à dimanche nuit et celles liées à dimanche journée. Cela peut s'expliquer par le fait que le dimanche n'est pas considéré comme un jour de travail en France, et que ce jours la les horaires des salariés sont plus allégés dans le cas où ils exercent leur profession.\n",
    "En dehors de ces jours, nous voyons bien qu'en semaine, les horaires de journée ont une forte corrélation entre eux et une faible corrélation avec les horaires de nuit. Et inversement, les horaires de nuit ont une forte corrélation entre eux et une faible corrélation avec les horaires de journée. Cela est cohérent étant donné qu'au cours de la semaine, la dynamique d'utilisation des vélib est assez similaire étant donné que les horaires de journée sont assez similaires en semaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18138f-919f-4cee-a9a6-c8d4ae4d6c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lundi = categorical_data.iloc[:, :24]  # Sélection des 24 premières colonnes\n",
    "mardi = categorical_data.iloc[:, 24:48] \n",
    "mercredi = categorical_data.iloc[:, 48:72] \n",
    "jeudi = categorical_data.iloc[:, 72:96] \n",
    "vendredi = categorical_data.iloc[:, 96:120] \n",
    "samedi = categorical_data.iloc[:, 120:144] \n",
    "dimanche = categorical_data.iloc[:, 144:168]\n",
    "\n",
    "lundi_nuit = pd.concat([lundi.iloc[:, 0:8], lundi.iloc[:, 20:24]], axis=1)\n",
    "lundi_journée = lundi.iloc[:, 8:20]\n",
    "#Pour mardi\n",
    "mardi_nuit = pd.concat([mardi.iloc[:, 0:8], mardi.iloc[:, 20:24]], axis=1)\n",
    "mardi_journée = mardi.iloc[:, 8:20]\n",
    "\n",
    "#Pour mercredi\n",
    "mercredi_nuit = pd.concat([mercredi.iloc[:, 0:7], mercredi.iloc[:, 20:24]], axis=1)\n",
    "mercredi_journée = mercredi.iloc[:, 8:20]\n",
    "#Pour jeudi\n",
    "jeudi_nuit = pd.concat([jeudi.iloc[:, 0:7], jeudi.iloc[:, 20:24]], axis=1)\n",
    "jeudi_journée = jeudi.iloc[:, 8:20]\n",
    "#Pour vendredi\n",
    "vendredi_nuit = pd.concat([vendredi.iloc[:, 0:7], vendredi.iloc[:, 20:24]], axis=1)\n",
    "vendredi_journée = vendredi.iloc[:, 8:20]\n",
    "#Pour samedi\n",
    "samedi_nuit = pd.concat([samedi.iloc[:, 0:7], samedi.iloc[:, 20:24]], axis=1)\n",
    "samedi_journée = samedi.iloc[:, 8:20]\n",
    "#Pour dimanche\n",
    "dimanche_nuit = pd.concat([dimanche.iloc[:, 0:7], dimanche.iloc[:, 20:24]], axis=1)\n",
    "dimanche_journée = dimanche.iloc[:, 8:20]\n",
    "# Liste des dataframes à fusionner\n",
    "dataframes = [lundi_nuit, lundi_journée, mardi_nuit, mardi_journée, mercredi_nuit, mercredi_journée,\n",
    "               jeudi_nuit, jeudi_journée, vendredi_nuit, vendredi_journée, samedi_nuit, samedi_journée,\n",
    "               dimanche_nuit, dimanche_journée]\n",
    "\n",
    "# Fusionner tous les dataframes en un seul dataframe\n",
    "result = pd.concat(dataframes, keys=['lundi_nuit', 'lundi_journée', 'mardi_nuit', 'mardi_journée', 'mercredi_nuit',\n",
    "                                     'mercredi_journée', 'jeudi_nuit', 'jeudi_journée', 'vendredi_nuit',\n",
    "                                     'vendredi_journée', 'samedi_nuit', 'samedi_journée', 'dimanche_nuit',\n",
    "                                     'dimanche_journée'], axis=1)\n",
    "\n",
    "# Réinitialiser les index\n",
    "result.reset_index(inplace=True)\n",
    "res = result.drop(columns=['index'])\n",
    "\n",
    "loading_quali = res.groupby(level=0, axis=1).first()  \n",
    "loading_quali.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e504b-4852-484d-ad9e-a0f30f231c15",
   "metadata": {},
   "source": [
    "# Etude de l'altitude des stations du jeu de donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b136f5-7c17-46a5-a44d-6f3e136aed89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Comparaison du nombre de station en altitude\n",
    "loading_hill   = loading_data[coord.bonus == 1]\n",
    "loading_valley = loading_data[coord.bonus == 0]\n",
    "\n",
    "size = [len(loading_hill), len(loading_valley)]\n",
    "labels = ['1', '0']\n",
    "\n",
    "plt.pie(size, labels = labels, autopct=\"%1.1f%%\", \n",
    "        colors = [sns.color_palette('tab20b')[-1],sns.color_palette('tab20b')[0]])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c91e24-26a8-498d-b60a-c3ed403a2224",
   "metadata": {},
   "source": [
    "Nous voyons sur ce graphique que 89.3% des stations de Paris sont en plaine tandis que 10.7% des stations de Paris sont sur des collines. Il y a ainsi, au sein de la ville de Paris, une majorité de stations étant en plaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569f812-90d3-4266-a3a3-dc814036365f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Comparaison du nombre de station en altitude de manière plus visuel (2 Dimensions).\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "sctrplt = plt.scatter(coord['longitude'],coord['latitude'],  \n",
    "                      c = coord['bonus'], cmap = sns.color_palette('tab20b', as_cmap=True))\n",
    "\n",
    "plt.xlabel('Longitude', fontsize = 10)\n",
    "plt.ylabel('Latitude', fontsize = 10)\n",
    "plt.title('Hilltop stations', fontsize = 15)\n",
    "plt.legend(handles = sctrplt.legend_elements()[0], labels = [\"No hill\", \"Hill\"], fontsize = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd06091-316c-43b0-bd79-22be0b7d29c2",
   "metadata": {},
   "source": [
    "Ce graphe nous donne une representation 2D des villes de Paris selon leur altitude. On observe visuellement la forte présente de station de vélib n'étant pas en altitude. On remarque quand même que les stations en alitude forme quelques petits groupes que l'on va retrouver au Nord, à l'Est. Les stations en altitude au Sud sont quant à elles plus dispersés que celles du Nord et de l'Est."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96251497-54f0-466a-9516-96e57bb053f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coord['hill'] = coord['bonus'].astype('category')\n",
    "\n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = \"carto-positron\",\n",
    "                        color = 'hill', \n",
    "                        color_discrete_map = {0:'midnightblue', 1:'plum'},\n",
    "                        labels = {0: \"hello\", 1: \"hi\"},\n",
    "                        zoom = 10, opacity = .9,\n",
    "                        title = 'Hilltop stations')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494a1b3-69d6-45e6-ad88-d913dddcab25",
   "metadata": {},
   "source": [
    "Cette carte interactive reprend ce qu'on a expliqué précedemment. Cependant, nous pouvons avoir les emplacements exacts. On observe que les stations en alitude se situent vers Montmartre et Belleville. Cela est cohérent quand on connait Paris et on sait que d'après Wikipédia : \"Belleville et Montmartre se disputent le point de nivellement le plus haut à Paris\" ( point culminant de Paris sur Wikipédia).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ee263-ae0b-4354-a56a-a3e51eeccb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtrer les stations avec Bonus == 1\n",
    "stations_bonus_1 = coord[coord['bonus'] == 1]\n",
    "\n",
    "# Afficher les indices et les noms des stations avec Bonus == 1\n",
    "print(\"Stations avec Bonus == 1 :\")\n",
    "for index, station in stations_bonus_1.iterrows():\n",
    "    print(\"Indice :\", index, \"- Nom :\", station['names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55f777-89fc-47cc-b9c8-fe8631d31710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 45\n",
    "loading_data = loading.to_numpy()\n",
    "print(coord)\n",
    "n_steps = len(loading_data[i])   # nombre de pas de temps observés\n",
    "time    = 24*7  # Plage de temps observée\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20, 6))\n",
    "\n",
    "plt.plot(loading_data[i])\n",
    "for j in range(1,7*24+1):\n",
    "    if j%24==0:\n",
    "        plt.vlines(x=j,colors='r',ymin=0,ymax=1)\n",
    "plt.xlabel('Heure')\n",
    "plt.ylabel('Ratio du nombre de vélib disponible')\n",
    "plt.title(coord['names'][i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6feb34-0f30-48e3-b0f4-6d357eedc18e",
   "metadata": {},
   "source": [
    "Pour une station en altitude comme Belleville Pré Saint Gervais, nous voyons que le chargement le matin et en journée est assez faible refletant le fait que les citoyens de la ville de Paris utilisent les vélibs en début de journée pour se rendre sur leurs lieux de travail. Le faible chargement de cette station en journée s'explique par le fait que cette station est en altitude et que cela est plus compliqué de redéposer des vélibs dans cette station. En outre on observe des pics de chargement la nuit venant du fait que certaines personnes ont pour rôle de ramener les velibs à leurs stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b683b-c3bf-477e-bd56-fb5969c532d8",
   "metadata": {},
   "source": [
    "Nous avons mené dans cette partie une analyse descriptive complète au cours de laquelle nous nous somme penchés en détail sur les caractéristiques des stations de la ville de Paris tels que le chargement en vélibs à des heures précise ou au cours de la semaine. Mais aussi, sur l'altitude de ces stations. \n",
    "A présent nous allons nous pencher sur une partie importante de notre projet qui est l'Analyse en Composantes Principales (ACP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57814447-dac6-4683-9476-454e743b6933",
   "metadata": {},
   "source": [
    "# Analyse en ACP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e3d71-a9f7-4da9-b475-2585d0230124",
   "metadata": {},
   "source": [
    "Au cours de cette partie nous allons nous intérésser à l'analyse en composantes principales qui est une méthode intéréssante dans notre cas au vu des grandes dimensions de nos données. En effet, on sait que cette méthode nous permet de réduire la dimension de nos données tout en gardant une bonne explication (de la variabilité) de nos données. L'objectif principal de l'ACP est de transformer l'ensemble de nos variables, possiblement corrélées, en un ensemble de variable non corrélées appelées composantes principales.\n",
    "Lorsque l'on se penchera sur nos ACP, nous prendrons soin de normaliser nos données. On a fait ce choix même si les données sont initialement à la même échelle. Si on ne normalise pas, les flèches représentant nos variables sur le cercle des corrélations seront très petites, elles auront une norme petite (signifiant qu'elles ne sont pas corrélées avec nos composantes principales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe813a-5fa5-467c-98e5-c5890444f7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Réalisation de l'analyse en composantes principales (PCA) sur loading\n",
    "\n",
    "k=10  ###dimension de ACP\n",
    "\n",
    "pca = PCA(n_components=k)\n",
    "load_scale = pd.DataFrame(scale(loading), columns = loading.columns)\n",
    "load_pca = pca.fit_transform(load_scale)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print('-----------------Critère du coude------------------')\n",
    "plt.plot(range(1, len(pca.explained_variance_) + 1), pca.explained_variance_, marker='o')\n",
    "plt.xlabel('Nombre de composantes principales')\n",
    "plt.ylabel('Variance expliquée')\n",
    "plt.title('Critère du coude')\n",
    "plt.show()\n",
    "print('--------------------------------')\n",
    "print('--- PCA ---')\n",
    "print('Initial dimension:', loading.shape)\n",
    "print('Dimension after projection:', load_pca.shape)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('--- Explained variance ---')\n",
    "for i in range(k):    \n",
    "    print('Component ',i+1,':', round(pca.explained_variance_[i],2), 'i.e.', round(100*pca.explained_variance_ratio_[i],2), '% of the total variance')\n",
    "\n",
    "loading_data_pca= pd.DataFrame({\n",
    "    \"Dim1\" : load_pca[:,0], \n",
    "    \"Dim2\" : load_pca[:,1],\n",
    "    \"names\" : coord[\"names\"],\n",
    "})\n",
    "print(np.max(load_pca[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d55a67-96a4-4e1e-9361-ae5a42497c02",
   "metadata": {},
   "source": [
    "Nous choisissons arbitrairement la valeur de K et prenons pour cette partie K=10, représentant la dimension de l'ACP. \n",
    "Tout d'abord, nous allons utiliser la méthode du coude, cette méthode nous permet de trouver la valeur optimale de K. Pour ce faire, nous allons regarder la figure représentant la variance expliquée en fonction du nombre de composantes principales, et allons nous intéresser au point où il y a une cassure, \"un coude\". \n",
    "\n",
    "Ce graphe nous montre la variance expliquée par chacune des composantes principales, obtenues lors de l'ACP. Nous voyons que la courbe est décroissante, ce qui est normal. En effet, l'objectif de la décomposition en composante principale est de trouver la direction selon laquelle les données varient le plus. Par conséquent, on comprend que la première composante principale est choisie afin de maximiser la variance de nos données lorsqu'elles sont projetées selon cette direction. De ce fait, les composantes principales suivantes expliqueront moins de variance étant données qu'elles sont toutes orthogonales entre elles et donc à la première composante principale qui explique le plus de variance.\n",
    "\n",
    "Nous observons que la première composante principale, explique 39,81 % des données, la seconde explique 23.5 % et la troisième explique 5.28 %. Nous choisirons K = 5 composantes principales, ce qui nous permettra d'expliquer  près de 76.22% de la variabilité de nos données, tout en ayant réduis nos dimensions de 168 à 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffb857-8b44-4cc9-a443-533996bb42c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "# Création de la figure et des axes\n",
    "fig = plt.figure(figsize=(25, 15))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "# Visualisation des valeurs propres\n",
    "ax1.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "ax1.set_xlabel('Composante principale')\n",
    "ax1.set_ylabel('Variance expliquée')\n",
    "\n",
    "# Deuxième subplot\n",
    "ax2.plot(pca.explained_variance_ratio_)\n",
    "ax2.set_title('Explained variance according to the dimension of the PCA space')\n",
    "ax2.set_xlabel('Nombre de composantes dans l\\'ACP')\n",
    "ax2.set_ylabel('Variance expliquée')\n",
    "\n",
    "# Troisième subplot\n",
    "ax3.scatter(load_pca[:,0], load_pca[:,2], alpha=0.5)\n",
    "ax3.axhline(y=0, color='r')\n",
    "ax3.set_title('Graphe des individus avec les composantes 1 et 3')\n",
    "ax3.set_xlabel('Composante principale 1')\n",
    "ax3.set_ylabel('Composante principale 3')\n",
    "\n",
    "# Quatrième subplot\n",
    "ax4.scatter(load_pca[:,0], load_pca[:,1], alpha=0.5)\n",
    "ax4.axhline(y=0, color='r')\n",
    "ax4.set_title('Graphe des individus avec les composantes 1 et 2')\n",
    "ax4.set_xlabel('Composante principale 1')\n",
    "ax4.set_ylabel('Composante principale 2')\n",
    "\n",
    "\n",
    "plt.suptitle('Analyse en composantes principales (PCA)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9849e6-2a78-4bc8-87e3-82ade7d009d7",
   "metadata": {},
   "source": [
    "Les 2 premiers graphiques représentent le pourcentage de variance expliquée selon le nombre de composantes principales. Ces graphiques nous confirment visuellement que nous devons garder K = 5 composantes principales, car au delà le gain de variance expliquée devient minime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fbf19-6491-4d02-bb19-88ac00c8c6c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Calculer le remplissage moyen par station\n",
    "RemplissageMoyenStation = loading.mean(axis=1)\n",
    "\n",
    "# Définir les catégories de remplissage en fonction de RemplissageMoyenStation\n",
    "hab = ['faible' if x <= 0.4 else 'moyen' if x <= 0.6 else 'élevé' for x in RemplissageMoyenStation]\n",
    "\n",
    "# Encoder les étiquettes en valeurs numériques\n",
    "label_encoder = LabelEncoder()\n",
    "hab_encoded = label_encoder.fit_transform(hab)\n",
    "\n",
    "# Créer un dictionnaire de couleurs pour chaque étiquette\n",
    "color_map = {'faible': 'blue', 'moyen': 'green', 'élevé': 'red'}\n",
    "colors = [color_map[label] for label in hab]\n",
    "\n",
    "# Créer un graphique de dispersion des individus colorés en fonction du remplissage moyen\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(load_pca[:,0], load_pca[:,1], c=colors)\n",
    "plt.title(\"Graphes des individus colorés suivant le remplissage moyen\", fontsize=25)\n",
    "plt.xlabel(\"Composante PC1\", fontsize=25)\n",
    "plt.ylabel(\"Composante PC2\", fontsize=25)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb43b62-d9ff-4c2b-af95-74715f3d9789",
   "metadata": {},
   "source": [
    "En colorant les individus selon leur taux de remplissage moyen (bleu = < 0.4 ; vert = entre 0.4 et 0.6 ; rouge = > 0.6), on constate que la première composante explique le taux de remplissage moyen des individus au cours de la semaine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b20b24-e49d-4052-bcc8-db1966eb3f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcul des coordonnées des composantes principales\n",
    "coord1_1 = pca.components_[0] * np.sqrt(pca.explained_variance_[0])\n",
    "coord2_1 = pca.components_[1] * np.sqrt(pca.explained_variance_[1])\n",
    "\n",
    "coord1_2 = pca.components_[0] * np.sqrt(pca.explained_variance_[0])\n",
    "coord2_2 = pca.components_[2] * np.sqrt(pca.explained_variance_[2])\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Premier subplot\n",
    "ax1 = axes[0]\n",
    "for i, j, nom in zip(coord1_1, coord2_1, loading.columns):\n",
    "    ax1.text(i, j, nom)\n",
    "    ax1.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "ax1.add_patch(plt.Circle((0, 0), radius=1, color='gray', fill=False))\n",
    "ax1.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "ax1.set_title('Composantes 1 et 2')\n",
    "\n",
    "# Deuxième subplot\n",
    "ax2 = axes[1]\n",
    "for i, j, nom in zip(coord1_2, coord2_2, loading.columns):\n",
    "    ax2.text(i, j, nom)\n",
    "    ax2.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "ax2.add_patch(plt.Circle((0, 0), radius=1, color='gray', fill=False))\n",
    "ax2.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "ax2.set_title('Composantes 1 et 3')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfe652-84af-4801-9ff9-953db0730dd5",
   "metadata": {},
   "source": [
    "Ces graphiques sont les cercles des corrélations, ils représentent chacuns la projections de nos variables sur le plan formé par les composantes 1 et 2 pour le cercle de gauche et par les composantes 1 et 3 pour le cercle de droite. Les composantes d'une station selon les axes sont les coefficients de corrélation entre la variable en question et la composante principale sur laquelle on souhaite projeter la flèche.\n",
    "\n",
    "Nous allons à présent nous pencher sur le cercle des corrélations des composantes 1 et 2. Tout d'abord, nous observons que les flèches présentent sur ce cercle sont toutes proches de l'extrémité droite du cercle, nous indiquant que les différentes variables ont une corrélation forte avec les deux premières composantes principales de l'ACP. De plus, les variables ont une valeur positive pour l'abscisse, signifiant que les variables et notre première composante principale varient similairement. En outre, on remarque que la très grande majorité de nos variables ont une abscisse étant supérieure à 0.5 et étant en moyenne vers 0.7, cela signifie qu'il y a une corrélation positive et forte de nos variable avec l'axe des abscisse.\n",
    "\n",
    "Ensuite, nous remarquons que nos variables ont toutefois une ordonnée positive ou négative, contrairement à leurs abscisses qui sont toutes positives. Cependant, il est difficile à l'aide de ce graphe de comprendre à quoi pourrait correspondre cette deuxième composante principale. En effet, le grand nombre de variable de notre jeu de données loading impact l'affichage et donc rend difficile nos interprétations.\n",
    "\n",
    "De ce fait, pour la suite de cette partie, nous allons nous pencher sur les data frame que nous avons construis au début du projet. En effet, nous nous pencherons d'abord sur le data frame où nous avons regroupé nos colonnes par tranches de 24h pour former des jours comme variable. Ensuite, nous nous pencherons sur celui où nous avons décomposer nos colonnes en horaires de journée et horaires de nuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab3605-9452-4962-a1fd-401f1f9e44fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Réalisation de l'analyse en composantes principales (PCA) sur df\n",
    "\n",
    "k=5  ###dimension de ACP\n",
    "\n",
    "\n",
    "df_scale = pd.DataFrame(scale(df), columns = df.columns)\n",
    "\n",
    "pca = PCA(n_components=k)\n",
    "df_scale_pca = pca.fit_transform(df_scale)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print('--- PCA ---')\n",
    "print('Initial dimension:', df.shape)\n",
    "print('Dimension after projection:', df_scale_pca.shape)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('--- Explained variance ---')\n",
    "for i in range(k):    \n",
    "    print('Component ',i+1,':', round(pca.explained_variance_[i],2), 'i.e.', round(100*pca.explained_variance_ratio_[i],2), '% of the total variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6e72a-1314-4340-9350-9b2826b27bba",
   "metadata": {},
   "source": [
    "A présent, nous allons prendre une valeur initiale de K valant 5. On remarque bien que les composantes principales ont un pourcentage d'explication de la variance différent que dans le cas où nous avions pris K=10. On rappel que l'on a initialement 7 variables car on utilise le data frame que l'on a crée contenant les 7 jours de la semaine comme variable. On a ici une réduction du nombre de dimensions de 7 à 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8edd8-178a-43eb-a76a-919fe14c5eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Visualisation des valeurs propres\n",
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Composante principale')\n",
    "plt.ylabel('Variance expliquée')\n",
    "\n",
    "# Deuxième subplot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Explained variance according to the dimension of the PCA space')\n",
    "plt.xlabel('Nombre de composantes dans l\\'ACP')\n",
    "plt.ylabel('Variance expliquée')\n",
    "\n",
    "# Troisième subplot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(df_scale_pca[:,0], df_scale_pca[:,2], alpha=0.5)\n",
    "plt.axhline(y = 0, color = 'r')\n",
    "plt.title('Graphe des individues avec les composantes 1 et 3')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 3')\n",
    "\n",
    "# Quatrième subplot\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(df_scale_pca[:,0], df_scale_pca[:,1], alpha=0.5)\n",
    "plt.axhline(y = 0, color = 'r')\n",
    "plt.title('Graphe des individues avec les composantes 1 et 2')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "\n",
    "plt.suptitle('Analyse en composantes principales (PCA)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecac45-cf50-4e56-b862-2b1b27845798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calcul des coordonnées des composantes principales\n",
    "coord1_1 = pca.components_[0] * np.sqrt(pca.explained_variance_[0])\n",
    "coord2_1 = pca.components_[1] * np.sqrt(pca.explained_variance_[1])\n",
    "\n",
    "coord1_2 = pca.components_[0] * np.sqrt(pca.explained_variance_[0])\n",
    "coord2_2 = pca.components_[2] * np.sqrt(pca.explained_variance_[2])\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Premier subplot\n",
    "ax1 = axes[0]\n",
    "for i, j, nom in zip(coord1_1, coord2_1, df.columns):\n",
    "    ax1.text(i, j, nom)\n",
    "    ax1.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "ax1.add_patch(plt.Circle((0, 0), radius=1, color='gray', fill=False))\n",
    "ax1.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "ax1.set_title('Composantes 1 et 2')\n",
    "\n",
    "# Deuxième subplot\n",
    "ax2 = axes[1]\n",
    "for i, j, nom in zip(coord1_2, coord2_2, df.columns):\n",
    "    ax2.text(i, j, nom)\n",
    "    ax2.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "ax2.add_patch(plt.Circle((0, 0), radius=1, color='gray', fill=False))\n",
    "ax2.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "ax2.set_title('Composantes 1 et 3')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f2568-902f-4a39-9ed9-dbd69737c2f9",
   "metadata": {},
   "source": [
    "Nous voyons sur le cercle de corrélation de gauche (avec les composantes 1 et 2) une symétrie de certains jours de la semaine par rapport au première axe. En effet, les jours Lundi, Mardi, Mercredi et Jeudi sont très proches. A l'inverse nous voyons que les jours du week end sont eux aussi très proches mais de l'autre côté de l'axe des abscisses. Nous pouvons en déduire que l'axe formé par la deuxième composante principale permet de mettre en évidence la différence entre les jours de la semaine et du week end. Le jour de la semaine vendredi se trouvant entre les deux blocs de jours ( celui formé par les autres jours de la semaine et celui formé par les jours du week end), peut s'expliquer par le fait que ce jour est un jour assez proche du week end. En effet, nous observons que les jours lundi et vendredi sont plus proches des variables des jours du week end puisque ce sont les jours de la semaine qui précède ( pour vendredi) et suit(pour lundi) le week end. A l'inverse, le jour mercredi étant le plus éloigné du week end car en milieu de semaine, se traduit sur le cercle des corrélations, par le fait que la variable mercredi est celle qui est la plus éloignée des variables samedi et dimanche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c277b19-c598-40b7-ba75-095c65acb6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k=5  ###dimension de ACP\n",
    "\n",
    "jours_scale = pd.DataFrame(scale(jours), columns = jours.columns)\n",
    "\n",
    "pca = PCA(n_components=k)\n",
    "jours_scale_pca = pca.fit_transform(jours_scale)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print('--- PCA ---')\n",
    "print('Initial dimension:', jours_scale.shape)\n",
    "print('Dimension after projection:', jours_scale_pca.shape)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('--- Explained variance ---')\n",
    "for i in range(k):    \n",
    "    print('Component ',i,':', round(pca.explained_variance_[i],2), 'i.e.', round(100*pca.explained_variance_ratio_[i],2), '% of the total variance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f3512-cfbb-4b1e-80fa-0e177e3ba9e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Dans cette ACP nous allons utilisé le data frame contenant les variables journée et nuit pour chaque jours de la semaine, d'où le fait que nous ayons 14 variables initialement ( 2 pour chacun des jours de la semaine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02d10a-ddd0-446a-9e09-436bde0b7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Visualisation des valeurs propres\n",
    "plt.figure(figsize=(25, 15))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Composante principale')\n",
    "plt.ylabel('Variance expliquée')\n",
    "\n",
    "# Deuxième subplot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Explained variance according to the dimension of the PCA space')\n",
    "plt.xlabel('Nombre de composantes dans l\\'ACP')\n",
    "plt.ylabel('Variance expliquée')\n",
    "\n",
    "# Troisième subplot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(jours_scale_pca[:,0], jours_scale_pca[:,2], alpha=0.5)\n",
    "plt.axhline(y = 0, color = 'r')\n",
    "plt.title('Graphe des individues avec les composantes 1 et 3')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 3')\n",
    "\n",
    "# Quatrième subplot\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(jours_scale_pca[:,0], jours_scale_pca[:,1], alpha=0.5)\n",
    "plt.axhline(y = 0, color = 'r')\n",
    "plt.title('Graphe des individues avec les composantes 1 et 2')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "\n",
    "plt.suptitle('Analyse en composantes principales (PCA)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40870b42-9af5-4c75-814b-94524d4be74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calcul des coordonnées des composantes principales\n",
    "coord1_1 = pca.components_[0] * np.sqrt(pca.explained_variance_[0])\n",
    "coord2_1 = pca.components_[1] * np.sqrt(pca.explained_variance_[1])\n",
    "\n",
    "coord1_2 = pca.components_[0] * np.sqrt(pca.explained_variance_[0])\n",
    "coord2_2 = pca.components_[2] * np.sqrt(pca.explained_variance_[2])\n",
    "\n",
    "# Création de la figure et des sous-graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Premier subplot\n",
    "ax1 = axes[0]\n",
    "for i, j, nom in zip(coord1_1, coord2_1, jours.columns):\n",
    "    ax1.text(i, j, nom)\n",
    "    ax1.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "ax1.add_patch(plt.Circle((0, 0), radius=1, color='gray', fill=False))\n",
    "ax1.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "ax1.set_title('Composantes 1 et 2')\n",
    "\n",
    "# Deuxième subplot\n",
    "ax2 = axes[1]\n",
    "for i, j, nom in zip(coord1_2, coord2_2, jours.columns):\n",
    "    ax2.text(i, j, nom)\n",
    "    ax2.arrow(0, 0, i, j, color = 'r', width = 0.0001)\n",
    "ax2.add_patch(plt.Circle((0, 0), radius=1, color='gray', fill=False))\n",
    "ax2.axis((-1.2, 1.2, -1.2, 1.2))\n",
    "ax2.set_title('Composantes 1 et 3')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed950cb2-b13c-4a69-b215-fbb96e610dc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nous rappelons que pour les variables nous avons pris comme convention : \n",
    "- horaires de nuit : 00h à 7h59 et 20h00 à 00h\n",
    "- horaires de journée : 8h00 à 19h59\n",
    "\n",
    "Pour la première composante principale on reprend ce qu'on a dis pour la première ACP (cette composante explique le chargement moyen en vélibs des stations).\n",
    "\n",
    "La deuxieme composante principale semble porter l'information de la plage horaire jour/nuit. En effet, les variables journée ( ne contenant pas les horaires de nuit) sont corrélées positivement avec la deuxième composante principale, tandis que, les variables nuit contenant les heures de nuit sont corrélées négativement avec la deuxième composante principale.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795e740-ffb1-4ebe-9eae-1e7abcf5091f",
   "metadata": {},
   "source": [
    "Maintenant que nous avons bien vu et compris ce qu'était l'ACP, nous allons à présent nous pencher sur les différentes méthodes de clusering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e7a34",
   "metadata": {},
   "source": [
    "# MCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c8908",
   "metadata": {},
   "source": [
    "### Création de Contingency Table et CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "from prince import MCA\n",
    "from prince import CA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50483ac4",
   "metadata": {},
   "source": [
    "On va commncer avec une analyse de correspondance entre la variable de jours et la variable de loading. Vu que cette dernière est quantitative, il faut la discretiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae649891",
   "metadata": {},
   "outputs": [],
   "source": [
    "loading.mean(axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462609b1",
   "metadata": {},
   "source": [
    "On peut définir une liste d'intervalles, qui contient des valeurs seuilles pour découper la gamme de données quantitatives de loading en segments distincts. On va utiliser les quantiles de loading moyen pour definir ces intervalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [0, 0.204574, 0.353352, 0.532460, 0.7, 1.]\n",
    "labels = ['très peu chargé', 'peu chargé', 'moyen', 'chargé', 'très chargé']\n",
    "\n",
    "# Appliquer la transformation\n",
    "load_quali = pd.DataFrame()\n",
    "for column in jours.columns:\n",
    "    load_quali[column] = pd.cut(jours[column], bins=intervals, labels=labels, right=False)\n",
    "\n",
    "\n",
    "load_quali.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ba0e9",
   "metadata": {},
   "source": [
    "On a utilisé les trois quartiles pour construire les 3 premiers intervalles. Ensuite, on a ajouté un seuil intermidiare entre $q_{75}$ et 1 qui vaut 0.7 pour considerer les stations qui sont trés chargés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a59260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes à considérer\n",
    "columns_to_consider = ['dimanche_nuit', 'dimanche_journée', 'jeudi_nuit', 'jeudi_journée', \n",
    "        'lundi_nuit', 'lundi_journée', 'mardi_nuit', 'mardi_journée', \n",
    "          'mercredi_nuit', 'mercredi_journée', 'samedi_nuit', 'samedi_journée', \n",
    "          'vendredi_nuit', 'vendredi_journée']\n",
    "# Création de la table de contingence\n",
    "contingency_table = pd.DataFrame()\n",
    "\n",
    "# Pour chaque colonne, compter le nombre d'occurrences de chaque modalité\n",
    "for col in columns_to_consider:\n",
    "    contingency_table[col] = load_quali[col].value_counts()\n",
    "\n",
    "# Affichage de la table de contingence\n",
    "contingency_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928fd8f",
   "metadata": {},
   "source": [
    "Ensuite, on peut construire à partir du dernier data frame, la contigency table entre la variable jour et la variable de loading discrétisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636edf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ansco = CA(n_components=4)\n",
    "\n",
    "# Ajustement du modèle aux données encodées\n",
    "ansco.fit(contingency_table)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(ansco.eigenvalues_summary)\n",
    "print(ansco.row_coordinates(contingency_table))\n",
    "print(ansco.column_coordinates(contingency_table))\n",
    "print(ansco.total_inertia_)\n",
    "\n",
    "# Récupérer les coordonnées des lignes et des colonnes\n",
    "row_coords = ansco.row_coordinates(contingency_table)\n",
    "col_coords = ansco.column_coordinates(contingency_table)\n",
    "\n",
    "# Plot des coordonnées des lignes (modalités de chargement)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(row_coords[0], row_coords[1], label='Modalités de chargement')\n",
    "\n",
    "# Plot des coordonnées des colonnes (jours de la semaine)\n",
    "plt.scatter(col_coords[0], col_coords[1], label='Jours de la semaine')\n",
    "\n",
    "# Ajouter des labels aux points\n",
    "for i, txt in enumerate(contingency_table.index):\n",
    "    plt.annotate(txt, (row_coords[0][i], row_coords[1][i]))\n",
    "\n",
    "for i, txt in enumerate(contingency_table.columns):\n",
    "    plt.annotate(txt, (col_coords[0][i], col_coords[1][i]))\n",
    "\n",
    "# Ajouter une légende\n",
    "plt.legend() \n",
    "\n",
    "# Ajouter des labels aux axes\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "# Afficher le scatter plot\n",
    "plt.title('Scatter plot des modalités de chargement et des jours de la semaine')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f86a0c",
   "metadata": {},
   "source": [
    "On voit que les 4 premiers axes expliquent 100% de données. On observe sur ce graphe que la plupart des périodes de la semaine sont proches d'une modalité de chargement, à l'exception de \"lundi_journée\". On peut en déduire qu'aucune tendance générale de chargement moyen ne se dégage pour le lundi entre 8h et 20h."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13eb68",
   "metadata": {},
   "source": [
    "## Lien avec la variable bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2d75e-1eb8-4cb5-b9e6-79575fd7d2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_quali['Hilltop']=coord['bonus']\n",
    "nan_count = load_quali['Hilltop'].isna().sum()\n",
    "print(\"Nombre de NaN dans la colonne 'Hilltop' :\", nan_count)\n",
    "\n",
    "load_quali.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76249b56-5c55-4f5b-b003-71ad60b38f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_quali['Hilltop'].fillna(0, inplace=True)\n",
    "load_quali['Hilltop'] = load_quali['Hilltop'].fillna(0).astype(bool)\n",
    "load_quali.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3300a",
   "metadata": {},
   "source": [
    "Nous approfondirons notre analyse en intégrant également la variable bonus pour examiner son association avec la modalité hilltop. Nous avons observé que l'ajout de la colonne hilltop entraînait la conversion de la première valeur en NaN, que nous avons rectifiée en la rétablissant à sa valeur initiale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624b342-7845-4539-97fd-914d4e2f3725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyse en composantes principales multiples (MCA)\n",
    "mca = prince.MCA(n_components=5)\n",
    "mca.fit(load_quali) \n",
    "\n",
    "display(mca.eigenvalues_summary)\n",
    "\n",
    "mca.scree_plot()\n",
    "\n",
    "mca.plot(\n",
    "    load_quali,\n",
    "    x_component=0,\n",
    "    y_component=1,\n",
    "    show_column_markers=True,\n",
    "    show_row_markers=True,\n",
    "    show_column_labels=False,\n",
    "    show_row_labels=False\n",
    ")\n",
    "\n",
    "def plot_mca(ax1=0, ax2=1, mca=mca, data=load_quali):\n",
    "    dataset = mca.transform(data)\n",
    "    dataset.reset_index(inplace=True)\n",
    "    sns.scatterplot(data = dataset,\n",
    "                  x = ax1, y = ax2\n",
    "                , alpha=.7)\n",
    "\n",
    "    plt.xlabel('Component {} — {:.2f}%'.format(ax1, mca.percentage_of_variance_[ax1]))\n",
    "    plt.ylabel('Component {} — {:.2f}%'.format(ax2, mca.percentage_of_variance_[ax2]))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_mca(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ed575",
   "metadata": {},
   "source": [
    "Lors de l'application de MCA nous constatons que les deux premiers axes expliquent seulement environ 20% des données, ce qui est relativement faible. Par conséquent, les conclusions tirées à partir de cette MCA pourraient manquer de fiabilité dans ce contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4fbc0-d751-414e-b81b-19ca83e762cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot des coordonnées des modalités\n",
    "mca.plot(\n",
    "    load_quali,\n",
    "    x_component=0,\n",
    "    y_component=1,\n",
    "    show_column_markers=True,\n",
    "    show_row_markers=False,\n",
    "    show_column_labels=False,\n",
    "    show_row_labels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b51a2",
   "metadata": {},
   "source": [
    "On a projeté les modalités sur l'espace par les deux composantes principales trouvées par MCA. On voit que les modalités \"Hilltop_False\" et \"Hilltop_True\" correspondent aux points de coordonnés (0,0) et (0,0.12). Vu que les deux modalités sont trop proches et que les deux premiers axes n'expliquent que 20% de données, on peut confirmer que la projection dans ce cas n'est pas fiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7a5a4-27b6-4ed2-809a-751ba24ed7a6",
   "metadata": {},
   "source": [
    "# K-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec3cf5-a555-48db-a162-e45b7380ec79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import colors\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b262b6-f85d-475f-83a9-6b659fcc828a",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons nous pencher sur les méthodes de clustering tel que k-means, GMM et CAH.Le but du clustering sera de regrouper les stations dans des groupes en fonctions des tendances de chargement sur une semaine.\n",
    "\n",
    "Dans un premier temps nous allons nous intérésser à la méthode k-means. La méthode k means permet de partitionner notre ensemble de données en K groupes, aussi appelés clusters. Ces groupes se formeront de la manière suivantes. Nous considérons K points au hasard dans nos données qui seront considérés comme étant les centroïdes initiaux. Ensuite, l'algorithme se chargera de placer chaque données dans les clusters correspondants, celui qui minimise la variance intra classe pour nos données. Il y aura des nouveaux calculs de centroides et donc des répartitions encore différentes des données dans nos K clusters, et ce processus se fera jusqu'à converger vers une solution.\n",
    "\n",
    "L'objectif de cette première méthode est de regrouper nos données en K groupes homogènes, où les données au sein de chaque groupe sont similaires entre elles mais différentes des autres clusters (inertie intra-classe faible, inertie inter-classe forte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4cf18-e8d0-4dd8-8cfd-536af6e99811",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_reduce = load_pca[:,:3]\n",
    "load_data=loading.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefbd36-6e1b-4224-9bba-9658c80df19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K=7\n",
    "kmeans_pca= KMeans(n_clusters=K,init='k-means++')\n",
    "clusters_pca = kmeans_pca.fit_predict(load_reduce)\n",
    "\n",
    "cmap = plt.get_cmap('Set3',K)\n",
    "plt.bar(*np.unique(clusters_pca, return_counts=True), color=cmap.colors)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "clusters_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f0801",
   "metadata": {},
   "source": [
    "On fixe k-means++ comme initialisation vu que cette méthode choisit les centroïdes de manière stratégique pour favoriser une convergence plus rapide et des clusters de meilleure qualité. Au lieu de sélectionner les centroïdes de façon aléatoire, k-means++ utilise une approche probabiliste qui privilégie les points éloignés les uns des autres. Cela contribue à réduire les risques de convergence vers un minimum local et améliore la qualité globale des clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f12494-f452-4a3d-9726-d0b938b85e47",
   "metadata": {},
   "source": [
    "Nous utiliserons la méthode k means ++ qui est une amélioration de la méthode k means car elle place les centroïdes initiaux de manière plus intelligente afin de mieux observer la structure de nos données et de converger vers une solution globale plus rapidement.\n",
    "\n",
    "Tous d'abord, nous commençons par choisir un nombre K=7 de clusters. \n",
    "Ce graphique nous montre l'effectif des différents clusters obtenus après avoir utiliser k means. Globalement, l'effectifs des clusters est homogènes, entre 100 et 250 pour chaque cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e99dad-9c4b-4854-8b4a-ffbef0ba05f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('Set3',K)\n",
    "kmeans_pca= KMeans(n_clusters=K,init='k-means++')\n",
    "clusters_pca = kmeans_pca.fit_predict(load_reduce)\n",
    "n_pixel_x = 41\n",
    "n_pixel_y = 29\n",
    "print(clusters_pca.shape)\n",
    "load_image = clusters_pca.reshape((n_pixel_x, n_pixel_y))\n",
    "\n",
    "# Visualisation des clusters\n",
    "plt.scatter(load_pca[:, 0], load_pca[:, 1], c=clusters_pca, cmap='viridis')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title('Visualisation des clusters avec KMeans')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebd353-5b74-49e6-802f-8e0b56940d50",
   "metadata": {},
   "source": [
    "Nous affichons ici le graphe des individus représentant la répartition des données au sein de nos 7 clusters dans le plan de l'ACP formé par les deux premières composantes. Ce graphe confirme bien les résultats obtenus précédemment, en effet, on note que les clusters ont des effectifs assez similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84efc5e-7b90-42bd-abde-2eae1bbb6f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.arange(0, 168)\n",
    "for i in range(K):\n",
    "    plt.plot(x, np.mean(load_data[clusters_pca == i], axis = 0), color=cmap.colors[i])\n",
    "    plt.title(\"Chargement moyen de la classe \"+str(i+1))\n",
    "    plt.xlabel(\"heure\")\n",
    "    plt.ylabel(\"Loading\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e236e-e336-41dc-b03a-da9bf596dac8",
   "metadata": {},
   "source": [
    "Nous affichons le chargement moyen des données selon leur cluster, en fonction de l'heure.\n",
    "\n",
    "On observe des tendances différentes pour chaque clusters. Les pics de remplissage sont généralement soit en pleine journée soit la nuit. Certains clusters sont formés par des stations peu remplies en week-end tandis que d'autres sont formés par des stations utilisées tous les jours de la semaine. On constate également une certaine périodicité entre les jours de la semaine (pics toujours au même moment pour un cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae9254-56a6-4d8b-8995-1b1cafff1018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_init='auto', max_iter=100, random_state=42)\n",
    "visualizer = KElbowVisualizer(kmeans, k=(1,11))\n",
    "\n",
    "visualizer.fit(load_reduce)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9be7e-1fac-4c9b-99c9-7b7c74d1005c",
   "metadata": {},
   "source": [
    "Lors de l'implémentation de k means précédente, nous avons choisi arbitrairement la valeur de K. Cependant, dans l'optique d'obtenir une valeur optimale du nombre de cluster K pour l'implémentation de k means, nous allons utiliser la méthode du Coude. Cette méthode nous permet d'obtenir la variance intra classe en fonction du nombre de cluster, le nombre optimal de cluster K se lira à l'endroit où il y a la présence d'une cassure sur notre courbe. Les graphe ci représente la variance intra classe en fonction du nombre de cluster, la méthode du coude nous permet donc d'y lire le nombre optimal de cluster K qui est ici K=3.\n",
    "Nous avons cependant choisi un nombre de cluster K = 5. Ce choix va être motivé dans la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49cddb2-94c0-4fb6-8435-809481b4d19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15,8))\n",
    "\n",
    "for k in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', max_iter=100, random_state=42)\n",
    "    q, mod = divmod(k, 2)\n",
    "    \n",
    "    # Create SilhouetteVisualizer instance with KMeans instance\n",
    "    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "    visualizer.fit(load_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de43567-f76f-4fc1-991d-13e954a4acfd",
   "metadata": {},
   "source": [
    "Le score silhouette correspond à la différence entre la séparation et la cohésion que l'on normalise par le maximum (entre la séparation et la cohésion). Plus le score silhouette est proche de 1 et mieux la classification est, tandis que, si le score silhouette est négatif, cela signifie qu'il y a une mauvais classification. On souhaite avoir un score silhouette proche de 1 étant donné que notre but est de maximiser la variance inter classe, ici la séparation des clusters, et de minimiser la variance intra classe, ici la cohésion des clusters.\n",
    "\n",
    "Ces graphiques représente les graphes du score silhouette de chacun des points pour différents nombre de clusters K. \n",
    "Dans notre cas, comme nous avons choisi précédemment le nombre optimal de cluster (K=5), nous analyserons seulement ce graphique. Sur cette figure nous voyons que les différents formes (une pour chaque cluster) ont une épaisseur différentes cela s'explique par la proportion d'individus des clusters. La flèche en pointillé rouge représente le coefficient silhouette moyen des individus, valant dans notre cas 0.24. Tous les clusters dépassant cette ligne en pointillé rouge nous indiquant que nous avons un bon clustering (une bonne cohésion et une bonne séparation). \n",
    "\n",
    "Les pics négatifs s'expliquent par le fait qu'il y a des chevauchements entre les points de différents clusters, de ce fait le coefficient silhouette est négatif puisque que certains points (modélisant les stations) sont plus proches des points présents dans les autres clusters. En outre, cela est inévitable au vu de la forme de nos données, nous ne pouvons pas éviter d'avoir des chevauchements entre les classes sur leurs bords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979baf3-bca8-48ea-8ba3-45b9e3b6a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5\n",
    "cmap = plt.get_cmap('Set3',K)\n",
    "kmeans_pca= KMeans(n_clusters=K,init='k-means++')\n",
    "clusters_pca = kmeans_pca.fit_predict(load_reduce)\n",
    "\n",
    "\n",
    "# Visualisation des clusters\n",
    "plt.scatter(load_pca[:, 0], load_pca[:, 1], c=clusters_pca, cmap='viridis')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title('Visualisation des clusters avec KMeans')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c6fbc-28fe-428a-9c69-3753d4186624",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous avons utiliser l'algorithme k means ++ pour obtenir nos clusters et avons projeté nos données sur un espace de dimension réduit à l'aide de l'ACP. Cette figure, nous donne une représentation des 5 clusters obtenus à l'aide de l'algorithme k means dans un plan formé par les deux premières composantes principales obtenus lors de l'ACP. Nous observons que nos clusters forment des groupes bien distincts. De plus nous observons que les clusters ont une proportion de stations assez similaire comme vu précédemment sur le graphe silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff520d5a-08e6-4e36-8682-ff6b2cc861c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "\n",
    "table = pd.crosstab(coord[\"bonus\"], clusters_pca)\n",
    "\n",
    "# Affichez la table de contingence\n",
    "print(table)\n",
    "\n",
    "# Tracez un mosaic plot en utilisant la bibliothèque statsmodels.graphics.mosaicplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "mosaic(table.stack(), title='Mosaic Plot entre les classes et la variable Bonus pour les collines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2cdf2f-c192-432e-9e8e-4b54932a9770",
   "metadata": {},
   "source": [
    "Les stations situées sur une colline sont globalement toutes concentrés dans 2 clusters. On peut supposer que ces stations partage la même tendance. Cependant, les clusters contenant des stations en altitude contiennent aussi d'autres stations. On peut en déduire que les stations en altitude n'ont pas de tendances exclusives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1101047-7145-46d1-a9c9-9f75351d7374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 168)\n",
    "for i in range(K):\n",
    "    plt.plot(x, np.mean(load_data[clusters_pca == i], axis = 0), color=cmap.colors[i])\n",
    "    plt.title(\"Chargement moyen de la classe \"+str(i))\n",
    "    plt.xlabel(\"heure\")\n",
    "    plt.ylabel(\"Loading\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdeca49-2f2c-412d-954c-cb336bea6b83",
   "metadata": {},
   "source": [
    "Nous affichons le chargement moyen des données selon leur cluster, en fonction de l'heure.\n",
    "\n",
    "Comme pour le cas avec 7 clusters, on observe des tendances différentes pour chaque clusters. Les pics de remplissage sont généralement soit en pleine journée soit la nuit. Certains clusters sont formés par des stations peu remplies en week-end tandis que d'autres sont formés par des stations utilisées tous les jours de la semaine. On constate également une certaine périodicité entre les jours de la semaine (pics toujours au même moment pour un cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5b9fa",
   "metadata": {},
   "source": [
    "### Différences de clustering effectué avec les données traités par l'ACP et données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6e082-590e-4895-9c31-f8f090e47010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = 30 \n",
    "\n",
    "hours = np.arange(h, 168, 24)\n",
    "cluster = clusters_pca\n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = cluster, color_continuous_scale = px.colors.sequential.Plasma_r, #size = load_per_hour,\n",
    "                        zoom  = 10, opacity = .9,\n",
    "                        title = 'Clustering ACP des stations avec données traités par l\\'ACP ')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b685d-2ea8-4516-a7c0-79f2b7415a39",
   "metadata": {},
   "source": [
    "Nous voyons que certains clusters correspondent à des zones géographiques précisent tandis que d'autres sont plus éparses. Nous pouvons également remarquer que certaines zones sont mélangés entre plusieurs clusters (par ex. au centre ville). Ces chevauchements entre clusters peuvent s'expliquer par le fait que certaines stations ont des profiles assez similaires, de ce fait, il n'est pas évident de savoir à quel cluster associer ces stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caafcbd",
   "metadata": {},
   "source": [
    "### Confirmation du choix de la dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061cbf6-09e5-46bd-9b2d-c70ffa6726d3",
   "metadata": {},
   "source": [
    "Pour chaque classe, nous allons :\n",
    "- Créer 4 groupes de 45 individus tirés aléatoirement\n",
    "- Afficher le chargement moyen de ces groupes\n",
    "\n",
    "Buts :\n",
    "- Vérifier que les clusters sont homogènes\n",
    "- Visualiser le comportement de chaque classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9d182-a182-42a2-bcd3-204eec275bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = 45  # nb d'individues\n",
    "n=4  #nb de groupes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Boucle sur les clusters\n",
    "for cluster in range(5):\n",
    "    \n",
    "    indices_cluster = np.where(clusters_pca == cluster)[0]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Sélection de p indices aléatoires\n",
    "        np.random.shuffle(indices_cluster)\n",
    "        indices_to_plot = indices_cluster[:p]\n",
    "        \n",
    "        # Calcul de la moyenne des chargements pour ces individus\n",
    "        mean_loading = np.mean(loading.iloc[indices_to_plot], axis=0)\n",
    "        \n",
    "        # Tracé de la moyenne des chargements\n",
    "        plt.plot(mean_loading, label=f\"Cluster {cluster}, Group {i+1}\", marker='o')\n",
    "\n",
    "\n",
    "    # Ajouter des légendes et des étiquettes\n",
    "    plt.xlabel(\"Composantes principales\")\n",
    "    plt.ylabel(\"Loading moyen\")\n",
    "    plt.title(\"Loading moyen pour classe \" + str(cluster))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf03c96-1446-4fcd-88c6-119c90f21fde",
   "metadata": {},
   "source": [
    "Nous avons choisi 4 échantillons de 45 individus pour chaque clusters et avons pris soin de comparer le chargement moyen de ces 4 échantillons d'individus. Nous voyons bien que les distributions sont très supperposées, nous confirmant l'homogénéité des chargements moyen des individus au sein de chaque cluster. Cependant, un des clusters semble avoir des chargements moyen plus erratique entre ses stations. On peut l'expliquer par le fait que l'amplitude de chargement moyen est plus faible. De ce fait, cela nous permet de confirmer que le choix de K=5 clusters semble cohérent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1a4f9",
   "metadata": {},
   "source": [
    "# Classification Ascendante Hiérarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0163a-746f-4023-a476-fe308439e24e",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons nous pencher sur la Classification Ascendante Hierarchique (CAH) qui est une méthode nous permettant de regrouper nos données en clusters hierarchiques. Nous nous appuierons sur nos résultats afin d'obtenir une représentation visuelle à l'aide des dendrogrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchical ascending classification (CAH)\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = pdist(loading, metric=\"euclidean\")\n",
    "\n",
    "# Clustering\n",
    "\n",
    "hclustcomplete = linkage(d, method=\"complete\")\n",
    "hclustsingle = linkage(d, method=\"single\")\n",
    "hclustaverage = linkage(d, method=\"average\")\n",
    "hclustward = linkage(d, method=\"ward\")\n",
    "\n",
    "# Visualisation des dendrogrammes\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "dendrogram(hclustsingle, labels=None)\n",
    "plt.title(\"Single Linkage\")\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "dendrogram(hclustcomplete, labels=None)\n",
    "plt.title(\"Complete Linkage\")\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Distance\")\n",
    "#representation\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "dendrogram(hclustaverage, labels=None)\n",
    "plt.title(\"Average Linkage\")\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "dendrogram(hclustward, labels=None)\n",
    "plt.title(\"ward Linkage\")\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe952ea7-3bdd-4a59-9f6f-b186b201ecdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nous allons tout d'abord nous pencher sur 4 méthodes de calculs de similarités entre clusters que sont single, complete, average linkage et ward, nous permettant de savoir à quels moments fusionner nos clusters. La méthode de calcul single linkage est une méthode mesurant la plus petite distance entre les points de deux clusters.\n",
    "\n",
    "La méthode de calcul complete linkage est une méthode mesurant la plus grande distance entre les points de deux clusters.\n",
    "\n",
    "La méthode de calcul average linkage est une méthode mesurant la distance moyenne entre les couples de points des deux clusters. Pour ce faire on va considérer un point dans un cluster, calculer les distances entre ce point et tous les autres points du deuxième cluster, calculer la distance moyenne et répeter cela pour tous les autres points.\n",
    "\n",
    "La méthode de calcul Ward est une méthode cherchant à fusionner les clusters en minimisant la variance intra-cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00341d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Liste des liaisons à tester\n",
    "linkages = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "# Création de sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Boucle sur chaque liaison\n",
    "for i, linkage in enumerate(linkages):\n",
    "    # Initialisation de l'algorithme de clustering\n",
    "    cah = AgglomerativeClustering(linkage=linkage)\n",
    "    visualizer = KElbowVisualizer(cah, k=(1, 11), ax=axes[i])\n",
    "    \n",
    "    # Ajustement des données et affichage\n",
    "    visualizer.fit(load_reduce)\n",
    "    visualizer.set_title(f'Elbow Method for linkage={linkage}')\n",
    "\n",
    "# Affichage global\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a0d02-06f9-49eb-8ed6-104fc553d046",
   "metadata": {},
   "source": [
    "Avec la méthode du coude, le nombre idéal de cluster pour Ward est K=4. Nous prendrons donc K=4 clusters pour cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850aa65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Création des quatre sous-graphiques\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "n_clusters = 4\n",
    "\n",
    "# Méthodes de liaison à utiliser\n",
    "linkages = ['single', 'average', 'complete', 'ward']\n",
    "\n",
    "# Boucle sur les sous-graphiques et les méthodes de liaison\n",
    "for i, linkage in enumerate(linkages):\n",
    "    # Création du modèle de CAH avec le nombre de clusters désiré\n",
    "    cah = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "    \n",
    "    # Effectuer la classification ascendante hiérarchique sur les données PCA\n",
    "    clusters = cah.fit_predict(load_pca)\n",
    "    \n",
    "    # Visualisation des clusters\n",
    "    sns.scatterplot(x=load_pca[:, 0], y=load_pca[:, 1], hue=clusters, palette='viridis', legend='full', ax=axs[i//2, i%2])\n",
    "    axs[i//2, i%2].set_title(f'CAH avec liaison {linkage.capitalize()}')\n",
    "    axs[i//2, i%2].set_xlabel('Composante Principale 1 (PC1)')\n",
    "    axs[i//2, i%2].set_ylabel('Composante Principale 2 (PC2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a71e7-9ca3-47cf-ac36-200d21126059",
   "metadata": {},
   "source": [
    "Nous avons affiché le graphe des individus pour chacune des 4 méthodes de claculs expliquées précédemment. Tout d'abord, nous voyons que les clusters ne sont pas du tout différenciable et bien représenté pour la méthode single linkage. Cela vient du fait que lorsque nous avons considérer le saut le plus haut pour couper notre dendrogramme, les clusters n'étaient pas homogènes dans leurs répartions. Les 3 autres méthodes de calculs ont quant à elles des clusters assez similaires. Cependant, on notera que l'on retrouvera plus de chevauchements pour les méthodes average et complete linkage que pour Ward. La meilleure méthode de calcul parmis les 4 semble donc être la méthode Ward linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa29a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un objet CAH avec le nombre de clusters désiré\n",
    "n_clusters = 4\n",
    "linkage='ward'\n",
    "cah = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkage)\n",
    "\n",
    "# Effectuer la classification ascendante hiérarchique sur les données après ACP (load_pca)\n",
    "clusters_ward = cah.fit_predict(load_pca)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=load_pca[:,0], y=load_pca[:,1], hue=clusters_ward, palette='viridis', legend='full')\n",
    "plt.title('Classification Ascendante Hiérarchique (CAH) avec un le lien '+ linkage)\n",
    "plt.xlabel('Composante Principale 1 (PC1)')\n",
    "plt.ylabel('Composante Principale 2 (PC2)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064b432-ef9c-49ed-bb0e-aa8420782ccc",
   "metadata": {},
   "source": [
    "Cette figure représente la projection de nos données sur le plan formé par nos deux premières composantes principales. Nous voyons que nos 4 clusters sont globalement bien différenciables, même si on observe de légers chevauchements entre les clusters. Nous allons comparer cette méthode par la suite avec les autres méthodes de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 30\n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = clusters_ward, color_continuous_scale = px.colors.sequential.Plasma_r, \n",
    "                        zoom  = 10, opacity = 1,\n",
    "                        title = 'Clustering des stations avec liason Ward')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26c27d-2f79-454c-8ee7-713c3df57be3",
   "metadata": {},
   "source": [
    "Un des clusters est facilement identifiable au centre de Paris. Un autre occupe une bonne partie du nord de la ville. Les autres zones semblent équiréparties entres les 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fd018-4f6f-4413-8a09-d05f885deec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "\n",
    "table = pd.crosstab(coord[\"bonus\"], clusters_ward)\n",
    "\n",
    "# Affichez la table de contingence\n",
    "print(table)\n",
    "\n",
    "# Tracez un mosaic plot \n",
    "plt.figure(figsize=(10, 6))\n",
    "mosaic(table.stack(), title='Mosaic Plot entre les classes et la variable Bonus pour les collines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17479085-72d5-4637-9bac-4ada2992d1be",
   "metadata": {},
   "source": [
    "Les stations en altitude appartiennent presque toutes au même cluster. On peut en déduire qu'elles présentent toutes une tendance de chargement similaire. Toutefois, ce cluster contient également des stations situées en plaines. La tendance de chargement des stations en altitude n'est donc pas exclusive à ces stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a80e8-4004-45c8-8d55-446b8619bd17",
   "metadata": {},
   "source": [
    "### Confirmation de la dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8881e-e414-41fe-99fc-7507e5d51279",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 45  # nb d'individues\n",
    "n= 4  #nb de groupes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Boucle sur les clusters\n",
    "for cluster in range(4):\n",
    "    \n",
    "    indices_cluster = np.where(clusters_ward == cluster)[0]\n",
    "    \n",
    "    for i in range(4):\n",
    "        # Sélection de p indices aléatoires\n",
    "        np.random.shuffle(indices_cluster)\n",
    "        indices_to_plot = indices_cluster[:p]\n",
    "        \n",
    "        # Calcul de la moyenne des chargements pour ces individus\n",
    "        mean_loading = np.mean(loading.iloc[indices_to_plot], axis=0)\n",
    "        \n",
    "        # Tracé de la moyenne des chargements\n",
    "        plt.plot(mean_loading, label=f\"Cluster {cluster}, Group {i+1}\", marker='o')\n",
    "\n",
    "\n",
    "    # Ajouter des légendes et des étiquettes\n",
    "    plt.xlabel(\"Composantes principales\")\n",
    "    plt.ylabel(\"Loading moyen\")\n",
    "    plt.title(\"Loading moyen pour classe \" + str(cluster))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2944b-0b05-4b81-af8a-1393cc9bd64c",
   "metadata": {},
   "source": [
    "On observe une homogénéité dans les chargements d'un cluster. Cependant, un des clusters semble avoir une plus grande variabilité dans les chargements de ses stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add9d06-e420-4bee-be48-47b2082c8138",
   "metadata": {},
   "source": [
    "Maintenant que nous avons vu la méthode CAH, nous allons nous intérésser à notre troisième et dernière méthode de clustering, GMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d6d6b",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe896f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbaa39b",
   "metadata": {},
   "source": [
    "Dans cette partie, nous nous intéressons à l'approche du mélange de modèles gaussiens, qui est une méthode utilisée en clustering mais assez différentes de la méthode k-means. En effet, pour la méthode k means nous imposions à chaque point d'appartenir à un cluster tandis que pour la Gaussian Mixture Model, nous allons attribuer à chacun des points une probabilités d'appartenance à chaque cluster. Ensuite, le point sera attibué au cluster pour lequel la probabilité d'appartenance est la plus grande.\n",
    "\n",
    "Cette méthode est utilisée pour le clustering des variables quantitatives. En effet, elle suppose que notre échantillon contient plusieurs distributions gaussiennes différentes, c'est-à-dire que chaque cluster suit une loi gaussienne définie par des paramètres différents des autres clusters. La méthode cherche alors à trouver ces paramètres en estimant la moyenne et la matrice de covariance empiriques de chaque clusters et ensuite maximisant la vraisemblance des données observées sous le modèle sous plusieurs itérations avec l'algorithme de EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423db013-4d15-4df8-bd37-72bfa4360b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
    "n_clusters = 3\n",
    "liste = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "# Parcourir la liste des types de covariance\n",
    "for i, cov_type in enumerate(liste):\n",
    "    # Créer un modèle GMM avec le type de covariance actuel\n",
    "    gmm = GaussianMixture(n_components=n_clusters, covariance_type=cov_type, n_init=10, init_params='k-means++')\n",
    "    clusters_gmm = gmm.fit_predict(load_pca)\n",
    "    \n",
    "    # Trouver les coordonnées des sous-graphiques\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    # Afficher le scatterplot sur le sous-graphique correspondant\n",
    "    sns.scatterplot(x=load_pca[:, 0], y=load_pca[:, 1], hue=clusters_gmm, palette='viridis', legend='full', ax=axs[row, col])\n",
    "    axs[row, col].set_title(f'GMM Clustering with {cov_type} covariance')\n",
    "    axs[row, col].set_xlabel('Composante Principale 1 (PC1)')\n",
    "    axs[row, col].set_ylabel('Composante Principale 2 (PC2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281b43f-780d-4e0e-b0f9-a113476e00ba",
   "metadata": {},
   "source": [
    "Dans cette partie nous nous sommes intéréssés aux différentes structures de covariance des composantes gaussiennes que nous utilisons pour la modélisation de nos clusters. Tout d'abord, nous voyons que la méthode 'tied' est la moins adaptée, ici, nous ne parvenons pas à identifier les clusters car ces derniers sont superposés. Ensuite, les méthodes 'diag' et 'full' présentent certes de meilleurs résultats car nous parvenons à identifier plus facilement les clusters que pour la méthode 'tied'. Cependant, nous pouvons constater que les clusters n'ont pas une répartition homogène et qu'il y a un chevauchement assez notable des points aux frontières des clusters. Enfin, la méthode 'sphérical' apparait comme étant la meilleur dans notre cas, puisque les clusters ont une répartition plus homogène que pour les autres méthodes et il y a certes de très légers chevauchements mais cela peut s'expliquer par la représentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Nombre maximal de clusters à tester\n",
    "k_max = 15\n",
    "\n",
    "# Initialisation des listes pour BIC et silhouette\n",
    "bic = []\n",
    "silhouette = []\n",
    "\n",
    "# Boucle pour tester différents nombres de clusters\n",
    "for k in range(2, k_max):\n",
    "    # Modèle GMM avec k clusters\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type=\"spherical\",init_params='k-means++', n_init=10)\n",
    "    \n",
    "    # Entraînement du modèle et calcul du BIC\n",
    "    gmm.fit(load_pca)\n",
    "    bic.append(gmm.bic(load_pca))\n",
    "    \n",
    "    # Prédiction des clusters et calcul de la silhouette\n",
    "    clusters_gmm = gmm.fit_predict(load_pca)\n",
    "    silhouette.append(silhouette_score(load_pca, clusters_gmm, metric='euclidean'))\n",
    "\n",
    "# Affichage des graphiques avec subplots\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Graphique pour la sélection du nombre de clusters avec BIC\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(2, k_max), bic, marker='o', linestyle='-')\n",
    "plt.title(\"Sélection du nombre de clusters avec BIC\")\n",
    "plt.xlabel(\"Nombre de clusters\")\n",
    "plt.ylabel(\"BIC\")\n",
    "\n",
    "# Graphique pour la sélection du nombre de clusters avec silhouette\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(2, k_max), silhouette, marker='o', linestyle='-')\n",
    "plt.title(\"Sélection du nombre de clusters avec silhouette\")\n",
    "plt.xlabel(\"Nombre de clusters\")\n",
    "plt.ylabel(\"Score de silhouette\")\n",
    "\n",
    "# Affichage global\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718ace2",
   "metadata": {},
   "source": [
    "On a essayé dans cette partie à chercher le nombre de cluster optimale pour GMM en utlisant deux critères différents : BIC et Silhouette. Pour le critère Silhouette, on cherche un intervalle ou l'inertie intra-classe est stable et on choisit le plus petit nombre de clusters correspondant à cet intervalle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f782aec",
   "metadata": {},
   "source": [
    "On fixe ici n = 5 clusters et on l'applique sur les données traitées par l'ACP afin de pouvoir afficher les résultats en fonction des 2 premières composantes principales. Comme l'estimation de la vraisemblance nécessite les paramètres de l'itération précédente, l'algorithme doit initialiser un modèle. Ici, cela est fait par défaut en utilisant KMeans++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# méthode GMM sur les données pca\n",
    "n_components = 5\n",
    "gmm = GaussianMixture(n_components = n_components, covariance_type=\"spherical\",init_params='k-means++', n_init=10).fit(load_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94574be5",
   "metadata": {},
   "source": [
    "Les composantes du mélange gaussien initiales seront initialisées en utilisant l'algorithme k-means++, ce qui signifie que les centres initiaux seront choisis de manière à maximiser la distance entre eux. l'algorithme est exécuté 10 fois avec différentes initialisations aléatoires. La meilleure solution finale sera sélectionnée parmi ces exécutions. Cela peut aider à éviter les problèmes de convergence vers des optimas locaux et à améliorer la qualité de la solution finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58137f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identification des classes\n",
    "classesGMM = gmm.predict(load_pca)\n",
    "\n",
    "# Effectifs des classes\n",
    "class_counts = pd.DataFrame(classesGMM).value_counts()\n",
    "\n",
    "# Plot des effectifs des classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Effectifs')\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('Histogramme des effectifs des classes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dd610",
   "metadata": {},
   "source": [
    "On voit que avec GMM, la répartion des individus entre les clusters est homogéne pour K = 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32919ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des clusters\n",
    "sns.scatterplot(x = load_pca[:, 0], y = load_pca[:, 1], hue=classesGMM, palette='viridis', legend='full')\n",
    "plt.title('Visualisation des clusters avec GMM')\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094818e",
   "metadata": {},
   "source": [
    "Ce graphique nous montre la distribution des stations dans 5 clusters obtenus à l'aide de la méthode GMM, dans le plan formé par les deux premières composantes principales de l'ACP. Nous retrouvons bien les commentaires effectués précédemment, puisque nous voyons un chevauchement de points assez conséquent entre certains clusters.\n",
    "\n",
    "Les résultats avec GMM sont plutôt satisfaisants étant donné que seulement certains points sont mélangés entre les clusters. \n",
    "\n",
    "#####Cela peut être due au fait que l'hypothése \"les distributions dans le data frame Load_pca sont guaussiennes\" n'est pas validée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 30\n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = classesGMM, color_continuous_scale = px.colors.sequential.Plasma_r, \n",
    "                        zoom  = 10, opacity = 1,\n",
    "                        title = 'Clustering des stations ')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b9ec9",
   "metadata": {},
   "source": [
    "On affiche les résultats de GMM sur une carte intéractive à l'aide de la librairie plotly.express\n",
    "\n",
    "Cette carte intéractive nous permet d'apprécier la répartitions des stations des différents clusters. Nous observons que les clusters sont séparés, ils ne forment pas 5 blocs compacts. On remarque aussi que GMM a fait une distinction entre les stations en altitude en les séparant majoritairement dans 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4152-8525-4f50-bcc8-7932a23aacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "\n",
    "table = pd.crosstab(coord[\"bonus\"], classesGMM)\n",
    "\n",
    "# Affichez la table de contingence\n",
    "print(table)\n",
    "\n",
    "# Tracez un mosaic plot \n",
    "plt.figure(figsize=(10, 6))\n",
    "mosaic(table.stack(), title='Mosaic Plot entre les classes et la variable Bonus pour les collines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50c704-97f9-4317-a3e0-7a2fc4e69c6a",
   "metadata": {},
   "source": [
    "On voit sur le mosaic plot que les stations en altitude ont majoritairement été placées dans 2 clusters différents. Mais ces 2 clusters ne sont pas entièrement composé de stations en altitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13161146-7d39-44e5-b58b-19a3600fe680",
   "metadata": {},
   "source": [
    "### Confirmation du choix de la dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f1d85-81f6-4933-baf5-739c83c19a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 45  # nb d'individues\n",
    "n= 4  #nb de groupes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Boucle sur les clusters\n",
    "for cluster in range(5):\n",
    "    \n",
    "    indices_cluster = np.where(classesGMM == cluster)[0]\n",
    "    \n",
    "    for i in range(4):\n",
    "        # Sélection de p indices aléatoires\n",
    "        np.random.shuffle(indices_cluster)\n",
    "        indices_to_plot = indices_cluster[:p]\n",
    "        \n",
    "        # Calcul de la moyenne des chargements pour ces individus\n",
    "        mean_loading = np.mean(loading.iloc[indices_to_plot], axis=0)\n",
    "        \n",
    "        # Tracé de la moyenne des chargements\n",
    "        plt.plot(mean_loading, label=f\"Cluster {cluster}, Group {i+1}\", marker='o')\n",
    "\n",
    "\n",
    "    # Ajouter des légendes et des étiquettes\n",
    "    plt.xlabel(\"Composantes principales\")\n",
    "    plt.ylabel(\"Loading moyen\")\n",
    "    plt.title(\"Loading moyen pour classe \" + str(cluster))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a528e57-e124-4d11-97f7-dbb0e9106e6d",
   "metadata": {},
   "source": [
    "On constate de manière similaire à KMeans des comportements homogènes au sein des clusters. Cependant on notera que l'un des clusters contenant les stations en altitudes a un comportement plus erratique. On peut peut-être l'expliquer par le fait que beaucoup de stations ont un chargement moyen assez faible au cours de la semaine sans pour antant partager la même tendance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbf264-f5c3-4678-9663-d4ca9444b7c2",
   "metadata": {},
   "source": [
    "### Comaparaison GMM et KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c2e8f-8301-4eb1-abe5-0625130f32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def plotKmeans(kmeans, data, n_clusters):\n",
    "    kmeans.fit(data)\n",
    "    clusters_kmeans = kmeans.predict(data)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.axis('equal')\n",
    "    cmap = plt.get_cmap('Set3', n_clusters)\n",
    "\n",
    "    # plot the input data\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=clusters_kmeans, s=1, linewidths=1, cmap=cmap)\n",
    "    \n",
    "    # plot the representation of the KMeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radius = [cdist(data[clusters_kmeans == i], [center]).max() for i, center in enumerate(centers)]\n",
    "    for i in range(n_clusters):\n",
    "        ax.add_patch(plt.Circle(centers[i], radius[i], fc=cmap.colors[i], alpha=0.3))\n",
    "\n",
    "\n",
    "def draw_ellipse(mean, covariance, alpha, ax, col='#CCCCCC'):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"    \n",
    "    # Convert covariance to principal axes\n",
    "    U, s, Vt = np.linalg.svd(covariance)\n",
    "    angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "    width, height = np.sqrt(s)\n",
    "\n",
    "    # Draw the Ellipse\n",
    "    ax.add_patch(Ellipse(mean, 2*width,2* height, angle=angle, alpha=alpha, fc=col))\n",
    "\n",
    "def plotGMM(gmm, data, n_clusters):\n",
    "    gmm.fit(data)\n",
    "    clusters_gmm = gmm.predict(data)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.axis('equal')\n",
    "    cmap = plt.get_cmap('Set3', n_clusters)\n",
    "    \n",
    "    # plot the input data\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=clusters_gmm, s=1, linewidths=1, cmap=cmap)\n",
    "    print(gmm.covariances_)\n",
    "    # w_factor = 0.2 / gmm.weights_.max()\n",
    "    for i in range(n_clusters):\n",
    "        mean = gmm.means_[i,:2]\n",
    "        covariance = gmm.covariances_[i,:,:2]\n",
    "        w = gmm.weights_[i]\n",
    "        draw_ellipse(mean, covariance, w, ax, cmap.colors[i])\n",
    "\n",
    "def draw_ellipse_sph(mean, variance, alpha, ax, col='#CCCCCC'):\n",
    "    \"\"\"Draw an ellipse with a given position and variance\"\"\"    \n",
    "    # Compute width and height from variance\n",
    "    width= height = 6 * np.sqrt(variance)\n",
    "\n",
    "    # Draw the Ellipse\n",
    "    ax.add_patch(Ellipse(mean, width, height, alpha=alpha, color=col))\n",
    "\n",
    "def plotGMM_sph(gmm, data, n_clusters):\n",
    "    gmm.fit(data)\n",
    "    clusters_gmm = gmm.predict(data)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.axis('equal')\n",
    "    cmap = plt.get_cmap('Set3', n_clusters)\n",
    "    \n",
    "    # plot the input data\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=clusters_gmm, s=1, linewidths=1, cmap=cmap)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        mean = gmm.means_[i,:2]\n",
    "        variance = gmm.covariances_[i]\n",
    "        w = gmm.weights_[i]\n",
    "        draw_ellipse_sph(mean, variance, w, ax, cmap.colors[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487d6ff-2eb0-408d-b94e-a13fd3c2d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters,init='k-means++', n_init='auto' )\n",
    "\n",
    "plotKmeans(kmeans, load_pca,n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1f053",
   "metadata": {},
   "source": [
    "On sait que Kmeans fait l'hypothèse que les clusters sont sphériques, donc dans notre cas, on visualise ces clusters avec la fonction plotKmeans qui crée un cercle pour chaque cluster dont le centre est le centroide corresopndant et de rayon qui est égale à la plus grande distance entre le centroide et un point de ce cluster. Cette représentation nous permet de constater que cette hypothèses rend Kmeans très sensible aux outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a998f25-d50f-4acb-8a4d-990bc3eab5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "gmm = GaussianMixture(n_components = n_components,covariance_type=\"spherical\",init_params='k-means++', n_init=10)\n",
    "\n",
    "plotGMM_sph(gmm, load_pca, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea2568-cab2-47a1-91c5-f7a96d584a6a",
   "metadata": {},
   "source": [
    "# Comparaison de 3 méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426ca3f-9e6a-472e-9645-fdc8f14b4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "n_clusters = 5\n",
    "\n",
    "\n",
    "# Gaussian Mixture Model (GMM)\n",
    "gmm = GaussianMixture(n_components=n_clusters, covariance_type=\"spherical\", n_init=10, init_params='k-means++')\n",
    "clusters_gmm = gmm.fit_predict(load_pca)\n",
    "sns.scatterplot(x=load_pca[:, 0], y=load_pca[:, 1], hue=clusters_gmm, palette='viridis', legend='full', ax=axs[0])\n",
    "axs[0].set_title('GMM Clustering')\n",
    "axs[0].set_xlabel('Composante Principale 1 (PC1)')\n",
    "axs[0].set_ylabel('Composante Principale 2 (PC2)')\n",
    "\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "clusters_kmeans = kmeans.fit_predict(load_pca)\n",
    "sns.scatterplot(x=load_pca[:, 0], y=load_pca[:, 1], hue=clusters_kmeans, palette='viridis', legend='full', ax=axs[1])\n",
    "axs[1].set_title('KMeans Clustering')\n",
    "axs[1].set_xlabel('Composante Principale 1 (PC1)')\n",
    "axs[1].set_ylabel('Composante Principale 2 (PC2)')\n",
    "\n",
    "# CAH avec liaison Ward\n",
    "cah_ward = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "clusters_cah_ward = cah_ward.fit_predict(load_pca)\n",
    "sns.scatterplot(x=load_pca[:, 0], y=load_pca[:, 1], hue=clusters_cah_ward, palette='viridis', legend='full', ax=axs[2])\n",
    "axs[2].set_title('CAH avec Liaison Ward')\n",
    "axs[2].set_xlabel('Composante Principale 1 (PC1)')\n",
    "axs[2].set_ylabel('Composante Principale 2 (PC2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90475f9d-ec9c-49a5-add0-cdaf89212609",
   "metadata": {},
   "source": [
    "A présent, nous allons comparer les 3 méthodes de clustering que nous avons étudiés. Tout d'abord, nous pouvons voir que la méthode de CAH est la moins efficace pour la séparation des clusters. En effet, nous voyons qu'il y a un chevauchement important des points entre les clusters, et que les frontières des différents clusters ne sont pas clairement identifiables contrairement aux méthodes K-means et Gaussian Mixture Model. Ensuite, nous pouvons voir que k means et GMM sont très efficaces pour la séparation des clusters. Cependant, on observe que K means permet une séparation plus prononcée des clusters, cela peut s'expliquer par le fait que k means est une méthode que l'on pourrait qualifier de hard clustering. En effet, pour la méthode k means les points sont \"forcés\" à appartenir à un cluster. A l'inverse pour la méthode GMM, que l'on pourrait qualifier de soft clustering, les points possèdent une probabilité d'appartenance à chacun des clusters.  En outre, on a vue que avec GMM nous arrivons à rentrer un peu plus dans les données qu'avec les autres méthodes vu que GMM arrive à faire une distinction entre les stations sur les collines et donc à identifier deux comportements différents entre ces stations.\n",
    "on peut aussi expliquer les légers chevauchements de points de la méthode GMM, par le fait que \" la représentation n'est pas optimale\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81656c78-9bc1-4418-9c2e-0511eaf95789",
   "metadata": {},
   "source": [
    "A présent, nous allons comparer sur la carte de Paris les 3 méthodes de clustering que nous avons étudiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b346cc-1e7a-415a-95a3-38628776b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 30 \n",
    "\n",
    "hours = np.arange(h, 168, 24)\n",
    "cluster = clusters_pca\n",
    "\n",
    "# --- # \n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = cluster, color_continuous_scale = px.colors.sequential.Plasma_r, #size = load_per_hour,\n",
    "                        zoom  = 10, opacity = .9,\n",
    "                        title = 'Clustering KMeans des stations avec données traités par l\\'ACP ')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 30 \n",
    "\n",
    "hours = np.arange(h, 168, 24)\n",
    "cluster = clusters_pca\n",
    "\n",
    "# --- # \n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = clusters_gmm, color_continuous_scale = px.colors.sequential.Plasma_r, #size = load_per_hour,\n",
    "                        zoom  = 10, opacity = .9,\n",
    "                        title = 'Clustering GMM des stations avec données traités par l\\'ACP ')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7304e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h = 30 \n",
    "\n",
    "hours = np.arange(h, 168, 24)\n",
    "cluster = clusters_pca\n",
    "\n",
    "# --- # \n",
    "\n",
    "fig = px.scatter_mapbox(coord, lat = 'latitude', lon = 'longitude', \n",
    "                        mapbox_style = 'open-street-map',\n",
    "                        color = clusters_cah_ward, color_continuous_scale = px.colors.sequential.Plasma_r, #size = load_per_hour,\n",
    "                        zoom  = 10, opacity = .9,\n",
    "                        title = 'Clustering CAH des stations avec données traités par l\\'ACP ')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ffe87-4474-4f34-a6a6-f9d2e51e6e92",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ad952-83ba-474f-8eb1-fcce7714b735",
   "metadata": {},
   "source": [
    "Nous avons atteint l'objectif initial qui était d'identifier des clusters significatifs dans nos données. Cependant nous avons observé qu'il était difficile d'assurer une homogénéité parfaite au sein des clusters. De plus, le format assez particulier de nos données nous a empêché d'avoir des clusters toujours bien séparés. En outre, nous avons vu que les méthodes de clustering dépendaient du contexte (ici Kmeans est très efficace contrairement à CAH) et qu'il n'est pas conseillé de suivre aveuglément les résultats de nos méthodes sans les remettre en question (cf. choix du nombre idéal de clusters).ers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
